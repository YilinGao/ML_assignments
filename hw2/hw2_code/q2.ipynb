{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compsci 571 Homework 2\n",
    "Question 2 Variable Importance in Trees and Random Forests\n",
    "Yilin Gao (yg95)\n",
    "Python 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from os import system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 6)\n",
      "(100, 6)\n"
     ]
    }
   ],
   "source": [
    "train = np.genfromtxt('train.csv', delimiter=',', skip_header=1)\n",
    "test = np.genfromtxt('test.csv', delimiter=',', skip_header=1)\n",
    "\n",
    "train_X = train[:, 0: -1]\n",
    "train_y = train[:, -1]\n",
    "\n",
    "test_X = test[:, 0: -1]\n",
    "test_y = test[:, -1]\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q2a1, decision stump based on the best split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_two_classes(y, y0 = 0, y1 = 1):\n",
    "    n0 = y[y == y0].shape[0]\n",
    "    n1 = y[y == y1].shape[0]\n",
    "    return n0, n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a tree node on variable X[split] and threshold = thre into 2 child nodes\n",
    "# parameter X: the feature matrix, shape = [n, p]\n",
    "# parameter y: the label vector, shape = [n, 1]\n",
    "# parameter split: the index for the splitting variable in the feature matrix, in [0, p)\n",
    "# parameter thre: the splitting threshold for the splitting variable\n",
    "# parameter y0: the actual value of one type of label\n",
    "# parameter y1: the actual value of the other type of label\n",
    "# return X_left: the feature matrix X of the subset of data with X[split] < thre, shape = [nl, p]\n",
    "# return y_left: the lable vector y of the subset of data with X[split] < thre, shape = [nl, 1]\n",
    "# return X_right: the feature matrix X of the subset of data with X[split] >= thre, shape = [nr, p]\n",
    "# return y_right: the lable vector y of the subset of data with X[split] >= thre, shape = [nr, 1]\n",
    "def split_binary_children(X, y, split, thre):\n",
    "    # left branch of the splitting node\n",
    "    X_left = X[X[:, split] < thre]\n",
    "    y_left = y[X[:, split] < thre]\n",
    "    # right branch of root\n",
    "    X_right = X[X[:, split] >= thre]\n",
    "    y_right = y[X[:, split] >= thre]\n",
    "    return X_left, y_left, X_right, y_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes gini index of a node with binary labels (0, 1)\n",
    "# parameter n0: number of data points of one category\n",
    "# parameter n1: number of data points of the other category\n",
    "# return the gini index in the node\n",
    "def gini(n0, n1):\n",
    "    n = n0 + n1\n",
    "    return 2 * n0 * n1 / (n * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the split split for a binary decision stump (level = 1)\n",
    "# using gini index (= 2 * p * (1-p)) as the splitting criteria\n",
    "# parameter X: the feature matrix, shape = [n, p]\n",
    "# parameter y: the label vector, shape = [n, 1]\n",
    "# parameter best_thre: the \"preset\" best splitting threshold for the best split variable (in binary case 0.5)\n",
    "# return best: the **index** for the best splitting variable in X, in [0, p)\n",
    "def best_split(X, y, best_thre):\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    best = -1\n",
    "    # root\n",
    "    n_root_0, n_root_1 = count_two_classes(y)\n",
    "    assert n == n_root_0 + n_root_1\n",
    "    root_gini = gini(n_root_0, n_root_1) \n",
    "    # the maximal gini index is the gini index at root\n",
    "    min_gini = root_gini\n",
    "    for j in range(0, p): # split on variable X[j] on root\n",
    "        X_left, y_left, X_right, y_right = split_binary_children(X.reshape(n, p), y.reshape(n, 1), j, best_thre)        \n",
    "        # left branch of root\n",
    "        n_left = y_left.shape[0]\n",
    "        n_left_0, n_left_1 = count_two_classes(y_left)\n",
    "        assert n_left == n_left_1 + n_left_0 \n",
    "        gini_left = gini(n_left_0, n_left_1)\n",
    "        # right branch of root\n",
    "        n_right = X_right.shape[0]\n",
    "        n_right_0, n_right_1 = count_two_classes(y_right)\n",
    "        assert n_right == n_right_0 + n_right_1 \n",
    "        gini_right = gini(n_right_0, n_right_1)\n",
    "        # gini after split\n",
    "        assert n == n_left + n_right\n",
    "        gini_j = n_left / n * gini_left + n_right / n * gini_right\n",
    "        if gini_j < min_gini:\n",
    "            best = j\n",
    "            min_gini = gini_j\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best split variable index is 0\n",
      "The best split variable is X[1]\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(1)\n",
    "# n_idx = np.random.choice(500, 400, replace = False)\n",
    "best = best_split(train_X, train_y, 0.5)\n",
    "print('The best split variable index is', best)\n",
    "print('The best split variable is X[' + str(best + 1) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relavent statistics of the decision stump are computed as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points in the left child: 243\n",
      "Number of points in the left child and y = 0: 209\n",
      "Number of points in the left child and y = 1: 34\n",
      "Number of points in the right child: 257\n",
      "Number of points in the right child and y = 0: 32\n",
      "Number of points in the right child and y = 1: 225\n",
      "Gini index before split: 0.499352\n",
      "Gini index in the left child: 0.24068146793341125\n",
      "Gini index in the right child: 0.21801995488198156\n"
     ]
    }
   ],
   "source": [
    "_, y_left, _, y_right = split_binary_children(train_X, train_y, best, 0.5)\n",
    "n_left = y_left.shape[0]\n",
    "n_right = y_right.shape[0]\n",
    "n_root_0, n_root_1 = count_two_classes(train_y)\n",
    "n_left_0, n_left_1 = count_two_classes(y_left)\n",
    "n_right_0, n_right_1 = count_two_classes(y_right)\n",
    "print('Number of points in the left child:', n_left)\n",
    "print('Number of points in the left child and y = 0:', n_left_0)\n",
    "print('Number of points in the left child and y = 1:', n_left_1)\n",
    "print('Number of points in the right child:', n_right)\n",
    "print('Number of points in the right child and y = 0:', n_right_0)\n",
    "print('Number of points in the right child and y = 1:', n_right_1)\n",
    "gini_root = gini(n_root_0, n_root_1)\n",
    "gini_left_0 = gini(n_left_0, n_left_1)\n",
    "gini_right_0 = gini(n_right_0, n_right_1)\n",
    "print('Gini index before split:', gini_root)\n",
    "print('Gini index in the left child:', gini_left_0)\n",
    "print('Gini index in the right child:', gini_right_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently we could use Sklearn package to compute the best decision stump:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best split using the sklearn package, for the picture\n",
    "dt = tree.DecisionTreeClassifier(max_depth = 1)\n",
    "dt = dt.fit(train_X, train_y)\n",
    "dotfile = open('tree_best_split.dot', 'w')\n",
    "tree.export_graphviz(dt, out_file = dotfile)\n",
    "dotfile.close()\n",
    "system('dot -Tpng tree_best_split.dot -o ../hw2_answer/images/tree_best_split.png')\n",
    "system('rm tree_best_split.dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q2a1, decision stump based on the best surrogate split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best surrogate split on the root for a given best split stump\n",
    "# parameter X: the feature matrix, shape = [n, p]\n",
    "# parameter best: the index for the best split variable in X, in [0, p)\n",
    "# parameter best_thre: the splitting threshold for the best split variable X[best]\n",
    "# parameter best_surr_thre: the \"preset\" splitting threshold for the best surrogate split variable (in binary case 0.5)\n",
    "# return best_surr: the **index** for the best surrogate split variable in X, in [0, p)\n",
    "def best_surrogate_split(X, best, best_thre, best_surr_thre):\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    assert best >= 0 and best < p\n",
    "    if p == 1: # no best surrogate split variable if only 1 variable in consideration\n",
    "        return -1\n",
    "    # pLbLj + pRbRj for all other variables that are not the best split variable\n",
    "    best_surr = -1\n",
    "    best_surr_sum = 0;\n",
    "    for j in range(0, p):\n",
    "        if j == best: # the best split variable\n",
    "            continue\n",
    "        plblj = X[np.logical_and(X[:, best] < best_thre, X[:, j] < best_surr_thre)].shape[0] / n\n",
    "        prbrj = X[np.logical_and(X[:, best] >= best_thre, X[:, j] >= best_surr_thre)].shape[0] / n\n",
    "        if plblj + prbrj > best_surr_sum:\n",
    "            best_surr = j\n",
    "            best_surr_sum = plblj + prbrj\n",
    "    return best_surr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best surrogate split variable index is 1\n",
      "The best surrogate split variable is X[2]\n"
     ]
    }
   ],
   "source": [
    "best_surr = best_surrogate_split(train_X, best, 0.5, 0.5)\n",
    "print('The best surrogate split variable index is', best_surr)\n",
    "print('The best surrogate split variable is X[' + str(best_surr + 1) + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on comparison, the best surrogate split for X1 is X2. The splitting threshold doesn't matter since X2 values are binary. Relavent statistics of the decision stump are computed as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points in the left child: 246\n",
      "Number of points in the left child and y = 0: 176\n",
      "Number of points in the left child and y = 1: 70\n",
      "Number of points in the right child: 254\n",
      "Number of points in the right child and y = 0: 65\n",
      "Number of points in the right child and y = 1: 189\n",
      "Gini index before split: 0.499352\n",
      "Gini index in the left child: 0.4071650472602287\n",
      "Gini index in the right child: 0.38083576167152333\n"
     ]
    }
   ],
   "source": [
    "_, y_left, _, y_right = split_binary_children(train_X, train_y, best_surr, 0.5)\n",
    "n_left = y_left.shape[0]\n",
    "n_right = y_right.shape[0]\n",
    "n_root_0, n_root_1 = count_two_classes(train_y)\n",
    "n_left_0, n_left_1 = count_two_classes(y_left)\n",
    "n_right_0, n_right_1 = count_two_classes(y_right)\n",
    "print('Number of points in the left child:', n_left)\n",
    "print('Number of points in the left child and y = 0:', n_left_0)\n",
    "print('Number of points in the left child and y = 1:', n_left_1)\n",
    "print('Number of points in the right child:', n_right)\n",
    "print('Number of points in the right child and y = 0:', n_right_0)\n",
    "print('Number of points in the right child and y = 1:', n_right_1)\n",
    "gini_root = gini(n_root_0, n_root_1)\n",
    "gini_left_1 = gini(n_left_0, n_left_1)\n",
    "gini_right_1 = gini(n_right_0, n_right_1)\n",
    "print('Gini index before split:', gini_root)\n",
    "print('Gini index in the left child:', gini_left_1)\n",
    "print('Gini index in the right child:', gini_right_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q2a2, 2 variable importance measures of all variables of the tree based on the best split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2703185497750236 0.10556222981883365\n"
     ]
    }
   ],
   "source": [
    "x1_importance_2 = gini_root - 243 / 500 * gini_left_0 - 257 / 500 * gini_right_0\n",
    "x2_importance_3 = gini_root - 246 / 500 * gini_left_1 - 254 / 500 * gini_right_1\n",
    "print(x1_importance_2, x2_importance_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q2a3, mean squares error of prediction on the test data of 2 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.27\n"
     ]
    }
   ],
   "source": [
    "# best split\n",
    "yhat_test_tree_best_split = np.ones(100)\n",
    "yhat_test_tree_best_split[test_X[:, best] == 0] = 0\n",
    "mse_test_tree_best_split = np.sum((yhat_test_tree_best_split - test_y) ** 2) / 100\n",
    "print(mse_test_tree_best_split)\n",
    "# best surrogate split\n",
    "yhat_test_tree_best_sur_split = np.ones(100)\n",
    "yhat_test_tree_best_sur_split[test_X[:, best_surr] == 0] = 0\n",
    "mse_test_tree_best_sur_split = np.sum((yhat_test_tree_best_sur_split - test_y) ** 2) / 100\n",
    "print(mse_test_tree_best_sur_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q2b1 grow random forest of decision stumps\n",
    "\n",
    "M = 1000 stumps\n",
    "\n",
    "B = 0.8 * n bootstrap training samples\n",
    "\n",
    "K = 1, 2, 3, 4, 5 random seleted variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter X: the feature matrix, shape = [n, p]\n",
    "# parameter y: the label vector, shape = [n, 1]\n",
    "# parameter M: number of stumps to generate in the forest\n",
    "# parameter b: bootstrap resample percentage\n",
    "# parameter K: number of randomly selected features in each stump\n",
    "# return best_dic:\n",
    "# return best_surr_dic:\n",
    "# return imp_5:\n",
    "# return imp_6:\n",
    "def random_forest(X, y, M, b, K):\n",
    "    np.random.seed(111)\n",
    "    best_dic = {}\n",
    "    best_surr_dic = {}\n",
    "    imp_5_dic = {}\n",
    "    imp_6_dic = {}\n",
    "    trees = np.empty([0, 3]) # split, left predict, right predict\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    n = X.shape[0]\n",
    "    B = int(round(b * n))\n",
    "    p = X.shape[1]\n",
    "    assert K <= p\n",
    "    for j in range(p): # initialize dics\n",
    "        best_dic[j] = 0\n",
    "        best_surr_dic[j] = 0\n",
    "        imp_5_dic[j] = np.empty(0)\n",
    "        imp_6_dic[j] = np.empty(0)\n",
    "    for m in range(M): # tree\n",
    "        n_idx = np.random.choice(n, B, replace = False) # indices for bootstrap samples\n",
    "        n_oobidx = list(set(range(n)) - set(n_idx))\n",
    "        feature_idx = np.random.choice(p, K, replace = False) # indices for selected features\n",
    "        y_sample = y[n_idx, :]\n",
    "        X_sample = X[n_idx, :]\n",
    "        X_sample = X_sample[:, feature_idx]\n",
    "        best_idx = best_split(X_sample.reshape(B, K), y_sample.reshape(B, 1), 0.5) # the \"false\" best split variable index in feature_idx\n",
    "        if best_idx == -1: # the best split variable doesn't exist\n",
    "            print('No best split when K = {}, m = {}'.format(K, m))\n",
    "            continue\n",
    "        best_surr_idx = best_surrogate_split(X_sample.reshape(B, K), best_idx, 0.5, 0.5) # the \"false\" best surrogate variable index in feature_idx\n",
    "        # update counter for best split variable and best surrogate variable       \n",
    "        best = feature_idx[best_idx] # the \"real\" best split variable index in X\n",
    "        best_dic[best] = best_dic[best] + 1\n",
    "        if best_surr_idx != -1: # the best surrogate splitting variable doesn't exist (K = 1)\n",
    "            best_surr = feature_idx[best_surr_idx] # the \"real\" best surrogate variable index in X\n",
    "            best_surr_dic[best_surr] = best_surr_dic[best_surr] + 1\n",
    "        # importance 5\n",
    "        _, y_left, _, y_right = split_binary_children(X_sample.reshape(B, K), y_sample.reshape(B, 1), best_idx, 0.5)\n",
    "        n_root = y_left.shape[0] + y_right.shape[0]\n",
    "        n_root_0, n_root_1 = count_two_classes(y_sample)\n",
    "        assert n_root == B\n",
    "        assert n_root == n_root_0 + n_root_1\n",
    "        n_left = y_left.shape[0]\n",
    "        n_right = y_right.shape[0]\n",
    "        n_left_0, n_left_1 = count_two_classes(y_left)\n",
    "        assert n_left == n_left_0 + n_left_1\n",
    "        n_right_0, n_right_1 = count_two_classes(y_right)\n",
    "        assert n_right == n_right_0 + n_right_1\n",
    "        gini_root = gini(n_root_0, n_root_1)\n",
    "        gini_left = gini(n_left_0, n_left_1)\n",
    "        gini_right = gini(n_right_0, n_right_1)\n",
    "        delta_gini = gini_root - n_left / n_root * gini_left - n_right / n_root * gini_right\n",
    "        imp_5_dic[best] = np.append(imp_5_dic[best], delta_gini)\n",
    "        # importance 6\n",
    "        if n_left_0 >= n_left / 2: # the left branch is predicted as 0\n",
    "            left_predict = 0\n",
    "            right_predict = 1\n",
    "        else:\n",
    "            left_predict = 1\n",
    "            right_predict = 0\n",
    "        trees = np.append(trees, np.array([best, left_predict, right_predict]).reshape(1, 3), axis = 0)\n",
    "        # out-of-bag\n",
    "        y_oob = y[n_oobidx, :]\n",
    "        X_oob = X[n_oobidx, :]\n",
    "        X_oob = X_oob[:, best]\n",
    "        # if X value < 0.5, goes to left, else goes to right\n",
    "        yhat_oob = np.ones(n - B)\n",
    "        if n_left_0 >= n_left / 2: # the left branch is predicted as 0\n",
    "            yhat_oob[X_oob < 0.5] = 0\n",
    "        else: # the right branch is predicted as 0\n",
    "            yhat_oob[X_oob >= 0.5] = 0\n",
    "        err_oob = np.sum((yhat_oob - y_oob.flatten()) ** 2) / (n - B)\n",
    "        # permute X[best]\n",
    "        np.random.shuffle(X_oob)\n",
    "        yhat_oob_perm = np.ones(n - B)\n",
    "        if n_left_0 >= n_left / 2: # the left branch is predicted as 0\n",
    "            yhat_oob_perm[X_oob < 0.5] = 0\n",
    "        else: # the right branch is predicted as 0\n",
    "            yhat_oob_perm[X_oob >= 0.5] = 0\n",
    "        err_oob_perm = np.sum((yhat_oob_perm - y_oob.flatten()) ** 2) / (n - B)\n",
    "        imp_6_dic[best] = np.append(imp_6_dic[best], err_oob_perm - err_oob)\n",
    "    return best_dic, best_surr_dic, imp_5_dic, imp_6_dic, trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q2b1\n",
    "\n",
    "For each K = 1, 2, 3, 4, 5:\n",
    "\n",
    "    how many times each variable is the best split\n",
    "    \n",
    "    how many times each variable is the best surrogate split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No best split when K = 1, m = 600\n",
      "K = 1:\n",
      "The map for best split variable is: {0: 215, 1: 203, 2: 186, 3: 192, 4: 203}\n",
      "The map for best surrogate split variable is: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}\n",
      "====================\n",
      "K = 2:\n",
      "The map for best split variable is: {0: 399, 1: 312, 2: 119, 3: 116, 4: 54}\n",
      "The map for best surrogate split variable is: {0: 0, 1: 106, 2: 291, 3: 268, 4: 335}\n",
      "====================\n",
      "K = 3:\n",
      "The map for best split variable is: {0: 585, 1: 321, 2: 48, 3: 39, 4: 7}\n",
      "The map for best surrogate split variable is: {0: 0, 1: 291, 2: 288, 3: 146, 4: 275}\n",
      "====================\n",
      "K = 4:\n",
      "The map for best split variable is: {0: 788, 1: 212, 2: 0, 3: 0, 4: 0}\n",
      "The map for best surrogate split variable is: {0: 0, 1: 582, 2: 168, 3: 22, 4: 228}\n",
      "====================\n",
      "K = 5:\n",
      "The map for best split variable is: {0: 1000, 1: 0, 2: 0, 3: 0, 4: 0}\n",
      "The map for best surrogate split variable is: {0: 0, 1: 1000, 2: 0, 3: 0, 4: 0}\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "best_dic_list = []\n",
    "imp_5_list = []\n",
    "imp_6_list = []\n",
    "trees_list = []\n",
    "for k in range(1, 6):\n",
    "    best_dic, best_surr_dic, imp_5_dic, imp_6_dic, trees = random_forest(train_X, train_y.reshape((500, 1)), 1000, 0.8, k)\n",
    "    best_dic_list.append(best_dic)\n",
    "    imp_5_list.append(imp_5_dic)\n",
    "    imp_6_list.append(imp_6_dic)\n",
    "    trees_list.append(trees)\n",
    "    print('K = ' + str(k) + ':') \n",
    "    print('The map for best split variable is:', best_dic)\n",
    "    print('The map for best surrogate split variable is:', best_surr_dic)\n",
    "    print('====================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q2b2\n",
    "\n",
    "For each k = 1, 2, 3, 4, 5:\n",
    "\n",
    "    compute 2 variable importance measures for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 1\n",
      "Variable importance (5) for variable 0 is 0.26941944916060806.\n",
      "Variable importance (6) for variable 0 is 0.37032558139534877.\n",
      "Variable importance (5) for variable 1 is 0.10559694666254214.\n",
      "Variable importance (6) for variable 1 is 0.22847290640394086.\n",
      "Variable importance (5) for variable 2 is 0.0009261358342768216.\n",
      "Variable importance (6) for variable 2 is -0.024193548387096784.\n",
      "Variable importance (5) for variable 3 is 0.000907843322142195.\n",
      "Variable importance (6) for variable 3 is 0.009479166666666664.\n",
      "Variable importance (5) for variable 4 is 0.0004051098267352955.\n",
      "Variable importance (6) for variable 4 is -0.033300492610837444.\n",
      "K = 2\n",
      "Variable importance (5) for variable 0 is 0.2702288467576328.\n",
      "Variable importance (6) for variable 0 is 0.3647117794486215.\n",
      "Variable importance (5) for variable 1 is 0.1061495545578077.\n",
      "Variable importance (6) for variable 1 is 0.22564102564102567.\n",
      "Variable importance (5) for variable 2 is 0.0012056344339151782.\n",
      "Variable importance (6) for variable 2 is -0.01915966386554623.\n",
      "Variable importance (5) for variable 3 is 0.0012664029458247744.\n",
      "Variable importance (6) for variable 3 is -0.01827586206896552.\n",
      "Variable importance (5) for variable 4 is 0.0006836187220307501.\n",
      "Variable importance (6) for variable 4 is -0.039999999999999994.\n",
      "K = 3\n",
      "Variable importance (5) for variable 0 is 0.2706398113802833.\n",
      "Variable importance (6) for variable 0 is 0.3638632478632478.\n",
      "Variable importance (5) for variable 1 is 0.10555813579225022.\n",
      "Variable importance (6) for variable 1 is 0.2277258566978193.\n",
      "Variable importance (5) for variable 2 is 0.0013225056790455963.\n",
      "Variable importance (6) for variable 2 is -0.021666666666666678.\n",
      "Variable importance (5) for variable 3 is 0.0013958773654121868.\n",
      "Variable importance (6) for variable 3 is -0.0235897435897436.\n",
      "Variable importance (5) for variable 4 is 0.0010133753609105148.\n",
      "Variable importance (6) for variable 4 is -0.03428571428571427.\n",
      "K = 4\n",
      "Variable importance (5) for variable 0 is 0.2703463899639109.\n",
      "Variable importance (6) for variable 0 is 0.3643908629441624.\n",
      "Variable importance (5) for variable 1 is 0.10590268518256904.\n",
      "Variable importance (6) for variable 1 is 0.23000000000000004.\n",
      "Variable importance (5) for variable 2 is nan.\n",
      "Variable importance (6) for variable 2 is nan.\n",
      "Variable importance (5) for variable 3 is nan.\n",
      "Variable importance (6) for variable 3 is nan.\n",
      "Variable importance (5) for variable 4 is nan.\n",
      "Variable importance (6) for variable 4 is nan.\n",
      "K = 5\n",
      "Variable importance (5) for variable 0 is 0.2706404814627808.\n",
      "Variable importance (6) for variable 0 is 0.36388.\n",
      "Variable importance (5) for variable 1 is nan.\n",
      "Variable importance (6) for variable 1 is nan.\n",
      "Variable importance (5) for variable 2 is nan.\n",
      "Variable importance (6) for variable 2 is nan.\n",
      "Variable importance (5) for variable 3 is nan.\n",
      "Variable importance (6) for variable 3 is nan.\n",
      "Variable importance (5) for variable 4 is nan.\n",
      "Variable importance (6) for variable 4 is nan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "for k in range(5):\n",
    "    imp_5 = imp_5_list[k]\n",
    "    imp_6 = imp_6_list[k]\n",
    "    print('K = {}'.format(k + 1))\n",
    "    for j in range(5): # variable\n",
    "        imp = imp_5[j].flatten()\n",
    "        imp = np.mean(imp)\n",
    "        print('Variable importance (5) for variable {} is {}.'.format(j, imp))\n",
    "        imp = imp_6[j].flatten()\n",
    "        imp = np.mean(imp)\n",
    "        print('Variable importance (6) for variable {} is {}.'.format(j, imp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q2b3\n",
    "\n",
    "compute the mean squares loss on the test data using 2 methods:\n",
    "\n",
    "1. use the majority vote of the stumps as the prediction\n",
    "\n",
    "2. find the prediction of each stump, compute squares loss on each, and average the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17 0.35823999999999867\n",
      "0.15 0.26290999999999953\n",
      "0.1 0.18998999999999874\n",
      "0.1 0.13603999999999788\n",
      "0.1 0.09999999999999859\n"
     ]
    }
   ],
   "source": [
    "for k in range(5):\n",
    "    best_dic = best_dic_list[k]\n",
    "    trees = trees_list[k]\n",
    "    M = trees.shape[0] # may not be 1000\n",
    "    yhat_test_votes = np.zeros(100) # count of vote for 1\n",
    "    err1 = 0\n",
    "    err2 = 0\n",
    "    for m in range(M):\n",
    "        split = int(trees[m, 0])\n",
    "        left_predict = trees[m, 1]\n",
    "        yhat_test = np.ones(100)\n",
    "        X_split = test_X[:, split]\n",
    "        if left_predict == 0:\n",
    "            yhat_test[X_split < 0.5] = 0\n",
    "        else:\n",
    "            yhat_test[X_split >= 0.5] = 0\n",
    "        yhat_test_votes = yhat_test_votes + yhat_test # if the current vote is 1, add 1 to yhat_test_votes count\n",
    "        err_tree = np.sum((yhat_test.flatten() - test_y) ** 2) / test_y.shape[0]\n",
    "        err2 = err2 + err_tree\n",
    "    yhat_test_votes[yhat_test_votes <= 500] = 0\n",
    "    yhat_test_votes[yhat_test_votes > 500] = 1\n",
    "    # numpy vector and 1d array are DIFFERENT!!!\n",
    "    err1 = np.sum((yhat_test_votes.flatten() - test_y) ** 2) / test_y.shape[0]\n",
    "    err2 = err2 / 1000\n",
    "    print(err1, err2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q2c grow random forest of decision stumps\n",
    "\n",
    "B = q * n, q = 0.4, 0.5, 0.6, 0.7, 0.8\n",
    "\n",
    "K = 2\n",
    "\n",
    "M = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q = 0.4:\n",
      "The map for best split variable is: {0: 406, 1: 283, 2: 103, 3: 104, 4: 104}\n",
      "The map for best surrogate split variable is: {0: 0, 1: 108, 2: 299, 3: 305, 4: 288}\n",
      "====================\n",
      "q = 0.5:\n",
      "The map for best split variable is: {0: 415, 1: 318, 2: 91, 3: 91, 4: 85}\n",
      "The map for best surrogate split variable is: {0: 0, 1: 107, 2: 305, 3: 275, 4: 313}\n",
      "====================\n",
      "q = 0.6:\n",
      "The map for best split variable is: {0: 392, 1: 301, 2: 103, 3: 128, 4: 76}\n",
      "The map for best surrogate split variable is: {0: 0, 1: 96, 2: 283, 3: 275, 4: 346}\n",
      "====================\n",
      "q = 0.7:\n",
      "The map for best split variable is: {0: 390, 1: 294, 2: 141, 3: 115, 4: 60}\n",
      "The map for best surrogate split variable is: {0: 0, 1: 103, 2: 275, 3: 270, 4: 352}\n",
      "====================\n",
      "q = 0.8:\n",
      "The map for best split variable is: {0: 399, 1: 312, 2: 119, 3: 116, 4: 54}\n",
      "The map for best surrogate split variable is: {0: 0, 1: 106, 2: 291, 3: 268, 4: 335}\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "best_dic_list = []\n",
    "imp_5_list = []\n",
    "imp_6_list = []\n",
    "trees_list = []\n",
    "for q in range(4, 9):\n",
    "    best_dic, best_surr_dic, imp_5_dic, imp_6_dic, trees = random_forest(train_X, train_y.reshape((500, 1)), 1000, q / 10, 2)\n",
    "    best_dic_list.append(best_dic)\n",
    "    imp_5_list.append(imp_5_dic)\n",
    "    imp_6_list.append(imp_6_dic)\n",
    "    trees_list.append(trees)\n",
    "    print('q = {}:'.format(q / 10)) \n",
    "    print('The map for best split variable is:', best_dic)\n",
    "    print('The map for best surrogate split variable is:', best_surr_dic)\n",
    "    print('====================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q2c1 variable importance in (5) and (6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q = 0.4\n",
      "Variable importance (5) for variable 0 is 0.2696168961880055.\n",
      "Variable importance (6) for variable 0 is 0.3669786535303777.\n",
      "Variable importance (5) for variable 1 is 0.10576785698493632.\n",
      "Variable importance (6) for variable 1 is 0.23422850412249704.\n",
      "Variable importance (5) for variable 2 is 0.003899756845279793.\n",
      "Variable importance (6) for variable 2 is -0.007961165048543696.\n",
      "Variable importance (5) for variable 3 is 0.0040542737776870135.\n",
      "Variable importance (6) for variable 3 is -0.0007692307692307742.\n",
      "Variable importance (5) for variable 4 is 0.002572356435299024.\n",
      "Variable importance (6) for variable 4 is -0.015064102564102563.\n",
      "q = 0.5\n",
      "Variable importance (5) for variable 0 is 0.2696419134600412.\n",
      "Variable importance (6) for variable 0 is 0.36537831325301207.\n",
      "Variable importance (5) for variable 1 is 0.10540532507373887.\n",
      "Variable importance (6) for variable 1 is 0.22784905660377358.\n",
      "Variable importance (5) for variable 2 is 0.0026316560853829815.\n",
      "Variable importance (6) for variable 2 is -0.005098901098901097.\n",
      "Variable importance (5) for variable 3 is 0.002545466099854059.\n",
      "Variable importance (6) for variable 3 is -0.008703296703296703.\n",
      "Variable importance (5) for variable 4 is 0.001845079099173983.\n",
      "Variable importance (6) for variable 4 is -0.01552941176470589.\n",
      "q = 0.6\n",
      "Variable importance (5) for variable 0 is 0.27305384821779055.\n",
      "Variable importance (6) for variable 0 is 0.3638520408163265.\n",
      "Variable importance (5) for variable 1 is 0.10467364172864807.\n",
      "Variable importance (6) for variable 1 is 0.22920265780730895.\n",
      "Variable importance (5) for variable 2 is 0.0018482054948763759.\n",
      "Variable importance (6) for variable 2 is -0.013398058252427188.\n",
      "Variable importance (5) for variable 3 is 0.0015440932535979264.\n",
      "Variable importance (6) for variable 3 is 0.0006250000000000092.\n",
      "Variable importance (5) for variable 4 is 0.0018482764129883317.\n",
      "Variable importance (6) for variable 4 is -0.025526315789473685.\n",
      "q = 0.7\n",
      "Variable importance (5) for variable 0 is 0.26901229613187644.\n",
      "Variable importance (6) for variable 0 is 0.3637948717948718.\n",
      "Variable importance (5) for variable 1 is 0.10599146218728056.\n",
      "Variable importance (6) for variable 1 is 0.22698412698412698.\n",
      "Variable importance (5) for variable 2 is 0.0016965422883942621.\n",
      "Variable importance (6) for variable 2 is -0.004349881796690308.\n",
      "Variable importance (5) for variable 3 is 0.0013330788043242373.\n",
      "Variable importance (6) for variable 3 is -0.006956521739130441.\n",
      "Variable importance (5) for variable 4 is 0.0009920541928565738.\n",
      "Variable importance (6) for variable 4 is -0.021555555555555547.\n",
      "q = 0.8\n",
      "Variable importance (5) for variable 0 is 0.2702288467576328.\n",
      "Variable importance (6) for variable 0 is 0.3647117794486215.\n",
      "Variable importance (5) for variable 1 is 0.1061495545578077.\n",
      "Variable importance (6) for variable 1 is 0.22564102564102567.\n",
      "Variable importance (5) for variable 2 is 0.0012056344339151782.\n",
      "Variable importance (6) for variable 2 is -0.01915966386554623.\n",
      "Variable importance (5) for variable 3 is 0.0012664029458247744.\n",
      "Variable importance (6) for variable 3 is -0.01827586206896552.\n",
      "Variable importance (5) for variable 4 is 0.0006836187220307501.\n",
      "Variable importance (6) for variable 4 is -0.039999999999999994.\n"
     ]
    }
   ],
   "source": [
    "for q in range(5):\n",
    "    imp_5 = imp_5_list[q]\n",
    "    imp_6 = imp_6_list[q]\n",
    "    print('q = {}'.format((q + 4) / 10))\n",
    "    for j in range(5): # variable\n",
    "        imp = imp_5[j].flatten()\n",
    "        imp = np.mean(imp)\n",
    "        print('Variable importance (5) for variable {} is {}.'.format(j, imp))\n",
    "        imp = imp_6[j].flatten()\n",
    "        imp = np.mean(imp)\n",
    "        print('Variable importance (6) for variable {} is {}.'.format(j, imp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q2c2 standard deviation of variable imporance in (5) and (6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q = 0.4\n",
      "Standard deviation of variable importance (5) for variable 0 is 0.02729892385445329.\n",
      "Standard deviation of variable importance (6) for variable 0 is 0.03144766695159181.\n",
      "Standard deviation of variable importance (5) for variable 1 is 0.022169085572470368.\n",
      "Standard deviation of variable importance (6) for variable 1 is 0.034970170757314345.\n",
      "Standard deviation of variable importance (5) for variable 2 is 0.003543632193578646.\n",
      "Standard deviation of variable importance (6) for variable 2 is 0.03305460491498919.\n",
      "Standard deviation of variable importance (5) for variable 3 is 0.003339324648230455.\n",
      "Standard deviation of variable importance (6) for variable 3 is 0.0387993710001409.\n",
      "Standard deviation of variable importance (5) for variable 4 is 0.002730278242917385.\n",
      "Standard deviation of variable importance (6) for variable 4 is 0.03365225881482412.\n",
      "q = 0.5\n",
      "Standard deviation of variable importance (5) for variable 0 is 0.02140012508633232.\n",
      "Standard deviation of variable importance (6) for variable 0 is 0.03714626267728457.\n",
      "Standard deviation of variable importance (5) for variable 1 is 0.017976038897554045.\n",
      "Standard deviation of variable importance (6) for variable 1 is 0.03998839276232263.\n",
      "Standard deviation of variable importance (5) for variable 2 is 0.002716865334182696.\n",
      "Standard deviation of variable importance (6) for variable 2 is 0.03708056418867786.\n",
      "Standard deviation of variable importance (5) for variable 3 is 0.002401705812326063.\n",
      "Standard deviation of variable importance (6) for variable 3 is 0.036586640961204085.\n",
      "Standard deviation of variable importance (5) for variable 4 is 0.001781237500571276.\n",
      "Standard deviation of variable importance (6) for variable 4 is 0.036882831406972495.\n",
      "q = 0.6\n",
      "Standard deviation of variable importance (5) for variable 0 is 0.017279324364130023.\n",
      "Standard deviation of variable importance (6) for variable 0 is 0.04133867766807827.\n",
      "Standard deviation of variable importance (5) for variable 1 is 0.015590602603902232.\n",
      "Standard deviation of variable importance (6) for variable 1 is 0.04407817872952716.\n",
      "Standard deviation of variable importance (5) for variable 2 is 0.001722073440157798.\n",
      "Standard deviation of variable importance (6) for variable 2 is 0.04882211416132113.\n",
      "Standard deviation of variable importance (5) for variable 3 is 0.0013763949593040767.\n",
      "Standard deviation of variable importance (6) for variable 3 is 0.048858053327982695.\n",
      "Standard deviation of variable importance (5) for variable 4 is 0.00224585046239385.\n",
      "Standard deviation of variable importance (6) for variable 4 is 0.04271677646650965.\n",
      "q = 0.7\n",
      "Standard deviation of variable importance (5) for variable 0 is 0.014476457556180272.\n",
      "Standard deviation of variable importance (6) for variable 0 is 0.04698745758479286.\n",
      "Standard deviation of variable importance (5) for variable 1 is 0.01252381833437205.\n",
      "Standard deviation of variable importance (6) for variable 1 is 0.04761163963541889.\n",
      "Standard deviation of variable importance (5) for variable 2 is 0.0013050293352929133.\n",
      "Standard deviation of variable importance (6) for variable 2 is 0.04592786213699263.\n",
      "Standard deviation of variable importance (5) for variable 3 is 0.0015380443446963374.\n",
      "Standard deviation of variable importance (6) for variable 3 is 0.05030067126719768.\n",
      "Standard deviation of variable importance (5) for variable 4 is 0.0008375723222486585.\n",
      "Standard deviation of variable importance (6) for variable 4 is 0.04959639572726836.\n",
      "q = 0.8\n",
      "Standard deviation of variable importance (5) for variable 0 is 0.010817989786650056.\n",
      "Standard deviation of variable importance (6) for variable 0 is 0.05649632043015585.\n",
      "Standard deviation of variable importance (5) for variable 1 is 0.009346760990509562.\n",
      "Standard deviation of variable importance (6) for variable 1 is 0.061801787037264085.\n",
      "Standard deviation of variable importance (5) for variable 2 is 0.0009634236339925176.\n",
      "Standard deviation of variable importance (6) for variable 2 is 0.058461792909036886.\n",
      "Standard deviation of variable importance (5) for variable 3 is 0.0009083532644132843.\n",
      "Standard deviation of variable importance (6) for variable 3 is 0.06589275327835123.\n",
      "Standard deviation of variable importance (5) for variable 4 is 0.0008125797188545062.\n",
      "Standard deviation of variable importance (6) for variable 4 is 0.06406015691288701.\n"
     ]
    }
   ],
   "source": [
    "for q in range(5):\n",
    "    imp_5 = imp_5_list[q]\n",
    "    imp_6 = imp_6_list[q]\n",
    "    print('q = {}'.format((q + 4) / 10))\n",
    "    for j in range(5): # variable\n",
    "        imp = imp_5[j]\n",
    "        std = np.std(imp)\n",
    "        print('Standard deviation of variable importance (5) for variable {} is {}.'.format(j, std))\n",
    "        imp = imp_6[j]\n",
    "        std = np.std(imp)\n",
    "        print('Standard deviation of variable importance (6) for variable {} is {}.'.format(j, std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
