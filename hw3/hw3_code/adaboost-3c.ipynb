{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting a decision stump\n",
    "\n",
    "The goal of this notebook is to implement your own boosting module.\n",
    "\n",
    "* Go through an implementation of decision trees.\n",
    "* Implement Adaboost ensembling.\n",
    "* Use your implementation of Adaboost to train a boosted decision stump ensemble.\n",
    "* Evaluate the effect of boosting (adding more decision stumps) on performance of the model.\n",
    "* Explore the robustness of Adaboost to overfitting.\n",
    "\n",
    "*This file is adapted from course material by Carlos Guestrin and Emily Fox.*\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## please make sure that the packages are updated to the newest version. \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a subset of the [LendingClub](https://www.kaggle.com/wendykan/lending-club-loan-data) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loans = pd.read_csv('loan_small.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recoding the target column\n",
    "\n",
    "We re-assign the target to have +1 as a safe (good) loan, and -1 as a risky (bad) loan. In the next cell, the features are also briefly explained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "\n",
    "loans['safe_loans'] = loans['loan_status'].apply(lambda x : +1 if x=='Fully Paid' else -1)\n",
    "\n",
    "## please update pandas to the newest version in order to execute the following line\n",
    "loans.drop(columns=['loan_status'], inplace=True)\n",
    "\n",
    "target = 'safe_loans' # this variable will be used later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform categorical data into binary features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will work with **binary decision trees**. Since all of our features are currently categorical features, we want to turn them into binary features using 1-hot encoding. \n",
    "\n",
    "We can do so with the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loans = pd.get_dummies(loans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the feature columns look like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['term_ 36 months',\n",
       " 'term_ 60 months',\n",
       " 'grade_A',\n",
       " 'grade_B',\n",
       " 'grade_C',\n",
       " 'grade_D',\n",
       " 'grade_E',\n",
       " 'grade_F',\n",
       " 'grade_G',\n",
       " 'home_ownership_MORTGAGE',\n",
       " 'home_ownership_NONE',\n",
       " 'home_ownership_OTHER',\n",
       " 'home_ownership_OWN',\n",
       " 'home_ownership_RENT',\n",
       " 'emp_length_1 year',\n",
       " 'emp_length_10+ years',\n",
       " 'emp_length_2 years',\n",
       " 'emp_length_3 years',\n",
       " 'emp_length_4 years',\n",
       " 'emp_length_5 years',\n",
       " 'emp_length_6 years',\n",
       " 'emp_length_7 years',\n",
       " 'emp_length_8 years',\n",
       " 'emp_length_9 years',\n",
       " 'emp_length_< 1 year']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = list(loans.columns)\n",
    "features.remove('safe_loans')  # Remove the response variable\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split\n",
    "\n",
    "We split the data into training and test sets with 80% of the data in the training set and 20% of the data in the test set. We use `seed=1` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(loans, test_size = 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data weights change as we build an AdaBoost model, we need to first code a decision tree that supports weighting of individual data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted error definition\n",
    "\n",
    "Consider a model with $N$ data points with:\n",
    "* Predictions $\\hat{y}_1 ... \\hat{y}_n$ \n",
    "* Target $y_1 ... y_n$ \n",
    "* Data point weights $\\alpha_1 ... \\alpha_n$.\n",
    "\n",
    "Then the **weighted error** is defined by:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$\n",
    "where $1[y_i \\neq \\hat{y_i}]$ is an indicator function that is set to $1$ if $y_i \\neq \\hat{y_i}$.\n",
    "\n",
    "\n",
    "### Write a function to compute weight of mistakes\n",
    "\n",
    "Write a function that calculates the weight of mistakes for making the \"weighted-majority\" predictions for a dataset. The function accepts two inputs:\n",
    "* `labels_in_node`: Targets $y_1 ... y_n$ \n",
    "* `data_weights`: Data point weights $\\alpha_1 ... \\alpha_n$\n",
    "\n",
    "We are interested in computing the (total) weight of mistakes, i.e.\n",
    "$$\n",
    "\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}].\n",
    "$$\n",
    "This quantity is analogous to the number of mistakes, except that each mistake now carries different weight. It is related to the weighted error in the following way:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$\n",
    "\n",
    "The function **intermediate_node_weighted_mistakes** should first compute two weights: \n",
    " * $\\mathrm{WM}_{-1}$: weight of mistakes when all predictions are $\\hat{y}_i = -1$ i.e $\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{-1}$)\n",
    " * $\\mathrm{WM}_{+1}$: weight of mistakes when all predictions are $\\hat{y}_i = +1$ i.e $\\mbox{WM}(\\mathbf{\\alpha}, \\mathbf{+1}$)\n",
    " \n",
    " where $\\mathbf{-1}$ and $\\mathbf{+1}$ are vectors where all values are -1 and +1 respectively.\n",
    " \n",
    "After computing $\\mathrm{WM}_{-1}$ and $\\mathrm{WM}_{+1}$, the function **intermediate_node_weighted_mistakes** should return the lower of the two weights of mistakes, along with the class associated with that weight. We have provided a skeleton for you with `YOUR CODE HERE` to be filled in several places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intermediate_node_weighted_mistakes(labels_in_node, data_weights):\n",
    "    # Sum the weights of all entries with label +1\n",
    "    total_weight_positive = sum(data_weights[labels_in_node == +1])\n",
    "    \n",
    "    # Weight of mistakes for predicting all -1's is equal to the sum above\n",
    "    ### YOUR CODE HERE\n",
    "    ...    \n",
    "    \n",
    "    # Sum the weights of all entries with label -1\n",
    "    ### YOUR CODE HERE\n",
    "    ...\n",
    "    \n",
    "    # Weight of mistakes for predicting all +1's is equal to the sum above\n",
    "    ### YOUR CODE HERE\n",
    "    ...\n",
    "    \n",
    "    # Return the tuple (weight, class_label) representing the lower of the two weights\n",
    "    #    class_label should be an integer of value +1 or -1.\n",
    "    # If the two weights are identical, return (weighted_mistakes_all_positive,+1)\n",
    "    ### YOUR CODE HERE\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Test your **intermediate_node_weighted_mistakes** function, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_labels = pd.Series([-1, -1, 1, 1, 1])\n",
    "example_data_weights = pd.Series([1., 2., .5, 1., 1.])\n",
    "if intermediate_node_weighted_mistakes(example_labels, example_data_weights) == (2.5, -1):\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# all data points}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to pick best feature to split on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to pick the best feature to split on.\n",
    "\n",
    "The **best_splitting_feature** function takes the data, the festures, the targetm and the data weights as input and returns the best feature to split on.\n",
    "  \n",
    "Complete the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If the data is identical in each feature, this function should return None\n",
    "\n",
    "def best_splitting_feature(data, features, target, data_weights):\n",
    "    \n",
    "    # These variables will keep track of the best feature and the corresponding error\n",
    "    best_feature = None\n",
    "    best_error = float('+inf') \n",
    "    num_points = float(len(data))\n",
    "\n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # The left split will have all data points where the feature value is 0\n",
    "        # The right split will have all data points where the feature value is 1\n",
    "        left_split = data[data[feature] == 0]\n",
    "        right_split = data[data[feature] == 1]\n",
    "        \n",
    "        # Apply the same filtering to data_weights to create left_data_weights, right_data_weights\n",
    "        ## YOUR CODE HERE\n",
    "        ...\n",
    "                    \n",
    "        # Calculate the weight of mistakes for left and right sides\n",
    "        ## YOUR CODE HERE\n",
    "        ...\n",
    "        \n",
    "        # Compute weighted error by computing\n",
    "        #  ( [weight of mistakes (left)] + [weight of mistakes (right)] ) / [total weight of all data points]\n",
    "        ## YOUR CODE HERE\n",
    "        ...\n",
    "        \n",
    "        # If this is the best error we have found so far, store the feature and the error\n",
    "        if error < best_error:\n",
    "            best_feature = feature\n",
    "            best_error = error\n",
    "    \n",
    "    # Return the best feature we found\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Now, we have another checkpoint to make sure you are on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_data_weights = np.array(len(train_data)* [1.5])\n",
    "if best_splitting_feature(train_data, features, target, example_data_weights) == 'term_ 36 months':\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside**. Relationship between weighted error and weight of mistakes:\n",
    "\n",
    "By definition, the weighted error is the weight of mistakes divided by the weight of all data points, so\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]}{\\sum_{i=1}^{n} \\alpha_i} = \\frac{\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\sum_{i=1}^{n} \\alpha_i}.\n",
    "$$\n",
    "\n",
    "In the code above, we obtain $\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ from the two weights of mistakes from both sides, $\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}})$ and $\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})$. First, notice that the overall weight of mistakes $\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ can be broken into two weights of mistakes over either side of the split:\n",
    "$$\n",
    "\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})\n",
    "= \\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\n",
    "= \\sum_{\\mathrm{left}} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\n",
    " + \\sum_{\\mathrm{right}} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\\\\\n",
    "= \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}}) + \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})\n",
    "$$\n",
    "We then divide through by the total weight of all data points to obtain $\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})\n",
    "= \\frac{\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}}) + \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the tree\n",
    "\n",
    "With the above functions implemented correctly, we are now ready to build our decision tree. A decision tree will be represented as a dictionary which contains the following keys:\n",
    "\n",
    "    { \n",
    "       'is_leaf'            : True/False.\n",
    "       'prediction'         : Prediction at the leaf node.\n",
    "       'left'               : (dictionary corresponding to the left tree).\n",
    "       'right'              : (dictionary corresponding to the right tree).\n",
    "       'features_remaining' : List of features that are posible splits.\n",
    "    }\n",
    "    \n",
    "Let us start with a function that creates a leaf node given a set of target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_leaf(target_values, data_weights):\n",
    "    \n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'is_leaf': True}\n",
    "    \n",
    "    # Computed weight of mistakes.\n",
    "    weighted_error, best_class = intermediate_node_weighted_mistakes(target_values, data_weights)\n",
    "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
    "    ## YOUR CODE HERE\n",
    "    ...\n",
    "    \n",
    "    return leaf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a function that learns a weighted decision tree recursively and implements 3 stopping conditions:\n",
    "1. All data points in a node are from the same class.\n",
    "2. No more features to split on.\n",
    "3. Stop growing the tree when the tree depth reaches **max_depth**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weighted_decision_tree_create(data, features, target, data_weights, current_depth = 1, max_depth = 10):\n",
    "    remaining_features = features[:] # Make a copy of the features.\n",
    "    target_values = data[target]\n",
    "    print \"--------------------------------------------------------------------\"\n",
    "    print \"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values))\n",
    "    \n",
    "    # Stopping condition 1. Error is 0.\n",
    "    if intermediate_node_weighted_mistakes(target_values, data_weights)[0] <= 1e-15:\n",
    "        print \"Stopping condition 1 reached.\"                \n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # Stopping condition 2. No more features.\n",
    "    if remaining_features == []:\n",
    "        print \"Stopping condition 2 reached.\"                \n",
    "        return create_leaf(target_values, data_weights)    \n",
    "    \n",
    "    # Additional stopping condition (limit tree depth)\n",
    "    if current_depth > max_depth:\n",
    "        print \"Reached maximum depth. Stopping for now.\"\n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # If all the datapoints are the same, splitting_feature will be None. Create a leaf\n",
    "    splitting_feature = best_splitting_feature(data, features, target, data_weights)\n",
    "    remaining_features.remove(splitting_feature)\n",
    "        \n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    right_split = data[data[splitting_feature] == 1]\n",
    "    \n",
    "    left_data_weights = data_weights[data[splitting_feature] == 0]\n",
    "    right_data_weights = data_weights[data[splitting_feature] == 1]\n",
    "    \n",
    "    print \"Split on feature %s. (%s, %s)\" % (\\\n",
    "              splitting_feature, len(left_split), len(right_split))\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if len(left_split) == len(data):\n",
    "        print \"Creating leaf node.\"\n",
    "        return create_leaf(left_split[target], data_weights)\n",
    "    if len(right_split) == len(data):\n",
    "        print \"Creating leaf node.\"\n",
    "        return create_leaf(right_split[target], data_weights)\n",
    "    \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    ## YOUR CODE HERE\n",
    "    \n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a recursive function to count the nodes in your tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following test code to check your implementation. Make sure you get **'Test passed'** before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Split on feature grade_A. (8775, 75)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (8775 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (75 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Split on feature grade_D. (19331, 3819)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (19331 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (3819 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_data_weights = np.array([1.0 for i in range(len(train_data))])\n",
    "small_data_decision_tree = weighted_decision_tree_create(train_data, features, target,\n",
    "                                        example_data_weights, max_depth=2)\n",
    "if count_nodes(small_data_decision_tree) == 7:\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'\n",
    "    print 'Number of nodes found:', count_nodes(small_data_decision_tree)\n",
    "    print 'Number of nodes that should be there: 7' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a quick look at what the trained tree is like. You should get something that looks like the following\n",
    "\n",
    "```\n",
    "{'is_leaf': False,\n",
    "    'left': {'is_leaf': False,\n",
    "        'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
    "        'prediction': None,\n",
    "        'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
    "        'splitting_feature': 'grade_A'\n",
    "     },\n",
    "    'prediction': None,\n",
    "    'right': {'is_leaf': False,\n",
    "        'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
    "        'prediction': None,\n",
    "        'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
    "        'splitting_feature': 'grade_D'\n",
    "     },\n",
    "     'splitting_feature': 'term. 36 months'\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_leaf': False,\n",
       " 'left': {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade_A'},\n",
       " 'prediction': None,\n",
       " 'right': {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade_D'},\n",
       " 'splitting_feature': 'term_ 36 months'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with a weighted decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give you a function that classifies one data point. It can also return the probability if you want to play around with that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):   \n",
    "    # If the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate: \n",
    "            print \"At leaf, predicting %s\" % tree['prediction']\n",
    "        return tree['prediction'] \n",
    "    else:\n",
    "        # Split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate: \n",
    "            print \"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value)\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the tree\n",
    "\n",
    "Now, we will write a function to evaluate a decision tree by computing the classification error of the tree on the given dataset.\n",
    "\n",
    "Again, recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# all data points}}\n",
    "$$\n",
    "\n",
    "The function called **evaluate_classification_error** takes in as input:\n",
    "1. `tree` (as described above)\n",
    "2. `data` (a dataframe)\n",
    "\n",
    "The function does not change because of adding data point weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classification_error(tree, data):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    # YOUR CODE HERE\n",
    "    ...\n",
    "    \n",
    "    # Once you've made the predictions, calculate the classification error\n",
    "    return (prediction != data[target]).sum() / float(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.390875"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Training a weighted decision tree\n",
    "\n",
    "To build intuition on how weighted data points affect the tree being built, consider the following:\n",
    "\n",
    "Suppose we only care about making good predictions for the **first 10 and last 10 items** in `train_data`, we assign weights:\n",
    "* 1 to the last 10 items \n",
    "* 1 to the first 10 items \n",
    "* and 0 to the rest. \n",
    "\n",
    "Let us fit a weighted decision tree with `max_depth = 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature emp_length_10+ years. (22413, 9587)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (22413 data points).\n",
      "Split on feature grade_A. (19673, 2740)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (19673 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (2740 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9587 data points).\n",
      "Stopping condition 1 reached.\n"
     ]
    }
   ],
   "source": [
    "# Assign weights\n",
    "example_data_weights = np.array([1.] * 10 + [0.]*(len(train_data) - 20) + [1.] * 10)\n",
    "\n",
    "# Train a weighted decision tree model.\n",
    "small_data_decision_tree_subset_20 = weighted_decision_tree_create(train_data, features, target,\n",
    "                         example_data_weights, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will compute the classification error on the `subset_20`, i.e. the subset of data points whose weight is 1 (namely the first and last 10 data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_20 = train_data.head(10).append(train_data.tail(10))\n",
    "evaluate_classification_error(small_data_decision_tree_subset_20, subset_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us compare the classification error of the model `small_data_decision_tree_subset_20` on the entire test set `train_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.445625"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree_subset_20, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model `small_data_decision_tree_subset_20` performs **a lot** better on `subset_20` than on `train_data`.\n",
    "\n",
    "So, what does this mean?\n",
    "* The points with higher weights are the ones that are more important during the training process of the weighted decision tree.\n",
    "* The points with zero weights are basically ignored during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing your own Adaboost (on decision stumps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a weighted decision tree working, it takes only a bit of work to implement Adaboost. For the sake of simplicity, let us stick with **decision tree stumps** by training trees with **`max_depth=1`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the lecture notes the procedure for Adaboost:\n",
    "\n",
    "1\\. Start with unweighted data with $\\alpha_j = 1$\n",
    "\n",
    "2\\. For t = 1,...T:\n",
    "  * Learn $f_t(x)$ with data weights $\\alpha_j$\n",
    "  * Compute coefficient $\\hat{w}_t$:\n",
    "     $$\\hat{w}_t = \\frac{1}{2}\\ln{\\left(\\frac{1- \\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}\\right)}$$\n",
    "  * Re-compute weights $\\alpha_j$:\n",
    "     $$\\alpha_j \\gets \\begin{cases}\n",
    "     \\alpha_j \\exp{(-\\hat{w}_t)} & \\text{ if }f_t(x_j) = y_j\\\\\n",
    "     \\alpha_j \\exp{(\\hat{w}_t)} & \\text{ if }f_t(x_j) \\neq y_j\n",
    "     \\end{cases}$$\n",
    "  * Normalize weights $\\alpha_j$:\n",
    "      $$\\alpha_j \\gets \\frac{\\alpha_j}{\\sum_{i=1}^{N}{\\alpha_i}} $$\n",
    "  \n",
    "Complete the skeleton for the following code to implement **adaboost_with_tree_stumps**. Fill in the places with `YOUR CODE HERE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "def adaboost_with_tree_stumps(data, features, target, num_tree_stumps):\n",
    "    # start with unweighted data (uniformly weighted)\n",
    "    alpha =  np.array([1.]*len(data))\n",
    "    weights = []\n",
    "    tree_stumps = []\n",
    "    target_values = data[target]\n",
    "    \n",
    "    for t in xrange(num_tree_stumps):\n",
    "        print '====================================================='\n",
    "        print 'Adaboost Iteration %d' % t\n",
    "        print '====================================================='        \n",
    "        # Learn a weighted decision tree stump. Use max_depth=1\n",
    "        # YOUR CODE HERE\n",
    "        ...\n",
    "        \n",
    "        # Make predictions\n",
    "        ## YOUR CODE HERE\n",
    "        ...\n",
    "        \n",
    "        # Produce a Boolean array indicating whether\n",
    "        # each data point was correctly classified\n",
    "        is_correct = predictions == target_values\n",
    "        is_wrong   = predictions != target_values\n",
    "        \n",
    "        # Compute weighted error\n",
    "        ## YOUR CODE HERE\n",
    "        ...\n",
    "        \n",
    "        # Compute model coefficient using weighted error\n",
    "        ## YOUR CODE HERE\n",
    "        ...\n",
    "        \n",
    "        # Adjust weights on data point\n",
    "        ## YOUR CODE HERE\n",
    "        adjustment = is_correct.apply(lambda is_correct : exp(-weight) if is_correct else exp(weight))\n",
    "        \n",
    "        # Scale alpha by multiplying by adjustment \n",
    "        # Then normalize data points weights\n",
    "        ## YOUR CODE HERE \n",
    "        ...\n",
    "    \n",
    "    return weights, tree_stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking your Adaboost code\n",
    "\n",
    "Train an ensemble of **two** tree stumps and see which features those stumps split on. We will run the algorithm with the following parameters:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, target, num_tree_stumps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stump(tree):\n",
    "    split_name = tree['splitting_feature'] # split_name is something like 'term. 36 months'\n",
    "    if split_name is None:\n",
    "        print \"(leaf, label: %s)\" % tree['prediction']\n",
    "        return None\n",
    "    split_feature, split_value = split_name.split('_')\n",
    "    print '                       root'\n",
    "    print '         |---------------|----------------|'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '  [{0} == 0]{1}[{0} == 1]    '.format(split_name, ' '*(27-len(split_name)))\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '    (%s)                 (%s)' \\\n",
    "        % (('leaf, label: ' + str(tree['left']['prediction']) if tree['left']['is_leaf'] else 'subtree'),\n",
    "           ('leaf, label: ' + str(tree['right']['prediction']) if tree['right']['is_leaf'] else 'subtree'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the first stump looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [term_ 36 months == 0]            [term_ 36 months == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(tree_stumps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the next stump looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade_A == 0]                    [grade_A == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(tree_stumps[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17198848113764034, 0.17728780637292607]\n"
     ]
    }
   ],
   "source": [
    "print stump_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your Adaboost is correctly implemented, the following things should be true:\n",
    "\n",
    "* `tree_stumps[0]` should split on **term. 36 months** with the prediction -1 on the left and +1 on the right.\n",
    "* `tree_stumps[1]` should split on **grade.A** with the prediction -1 on the left and +1 on the right.\n",
    "* Weights should be approximately `[0.158, 0.177]` \n",
    "\n",
    "**Reminders**\n",
    "- Stump weights ($\\mathbf{\\hat{w}}$) and data point weights ($\\mathbf{\\alpha}$) are two different concepts.\n",
    "- Stump weights ($\\mathbf{\\hat{w}}$) tell you how important each stump is while making predictions with the entire boosted ensemble.\n",
    "- Data point weights ($\\mathbf{\\alpha}$) tell you how important each data point is while training a decision stump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a boosted ensemble of 10 stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train an ensemble of 10 decision tree stumps with Adaboost. We run the **adaboost_with_tree_stumps** function with the following parameters:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_D. (26027, 5973)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26027 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5973 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_B. (23457, 8543)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23457 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8543 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (16870, 15130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16870 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (15130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, \n",
    "                                target, num_tree_stumps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "Recall from the lecture that in order to make predictions, we use the following formula:\n",
    "$$\n",
    "\\hat{y} = sign\\left(\\sum_{t=1}^T \\hat{w}_t f_t(x)\\right)\n",
    "$$\n",
    "\n",
    "We need to do the following things:\n",
    "- Compute the predictions $f_t(x)$ using the $t$-th decision tree\n",
    "- Compute $\\hat{w}_t f_t(x)$ by multiplying the `stump_weights` with the predictions $f_t(x)$ from the decision trees\n",
    "- Sum the weighted predictions over each stump in the ensemble.\n",
    "\n",
    "Complete the following skeleton for making predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_adaboost(stump_weights, tree_stumps, data):\n",
    "    scores = np.array([0.]*len(data))\n",
    "    \n",
    "    for i, tree_stump in enumerate(tree_stumps):\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x), axis = 1)\n",
    "        \n",
    "        # Accumulate predictions on scaores array\n",
    "        # YOUR CODE HERE\n",
    "        ...\n",
    "        \n",
    "    return scores.apply(lambda score : +1 if score > 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 10-component ensemble = 0.62825\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_adaboost(stump_weights, tree_stumps, test_data)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_data[target], predictions)\n",
    "print 'Accuracy of 10-component ensemble = %s' % accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us take a quick look what the `stump_weights` look like at the end of each iteration of the 10-stump ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17198848113764034,\n",
       " 0.17728780637292607,\n",
       " 0.10308067696970527,\n",
       " 0.08686702058343085,\n",
       " 0.07220085937815596,\n",
       " 0.07438562925268018,\n",
       " 0.058345528732096044,\n",
       " 0.04545487026453677,\n",
       " 0.03194548459992019,\n",
       " 0.02330529243219865]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stump_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** i: Are the weights monotonically decreasing, monotonically increasing, or neither?\n",
    "\n",
    "**Reminder**: Stump weights ($\\mathbf{\\hat{w}}$) tell you how important each stump is while making predictions with the entire boosted ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance plots\n",
    "\n",
    "In this section, we will try to reproduce some performance plots.\n",
    "\n",
    "### How does accuracy change with adding stumps to the ensemble?\n",
    "\n",
    "We will now train an ensemble with:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 30`\n",
    "\n",
    "Once we are done with this, we will then do the following:\n",
    "* Compute the classification error at the end of each iteration.\n",
    "* Plot a curve of classification error vs iteration.\n",
    "\n",
    "First, lets train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_D. (26027, 5973)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26027 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5973 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_B. (23457, 8543)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23457 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8543 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (16870, 15130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16870 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (15130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 10\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 11\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 12\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature emp_length_10+ years. (22413, 9587)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (22413 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9587 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 13\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_B. (23457, 8543)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23457 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8543 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 14\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 15\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_D. (26027, 5973)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26027 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5973 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 16\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 17\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 18\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 19\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_C. (23388, 8612)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23388 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8612 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 20\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (16870, 15130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16870 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (15130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 21\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 22\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 23\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_B. (23457, 8543)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23457 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8543 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 24\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature emp_length_2 years. (29104, 2896)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (29104 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (2896 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 25\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_G. (31657, 343)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (31657 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (343 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 26\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 27\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_G. (31657, 343)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (31657 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (343 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 28\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature home_ownership_OWN. (29204, 2796)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (29204 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (2796 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 29\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_G. (31657, 343)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (31657 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (343 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "# this may take a while... \n",
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, \n",
    "                                 features, target, num_tree_stumps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing training error at the end of each iteration\n",
    "\n",
    "Now, we will compute the classification error on the **train_data** and see how it is reduced as trees are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, training error = 0.41484374999999996\n",
      "Iteration 2, training error = 0.43281250000000004\n",
      "Iteration 3, training error = 0.39059374999999996\n",
      "Iteration 4, training error = 0.39059374999999996\n",
      "Iteration 5, training error = 0.37931250000000005\n",
      "Iteration 6, training error = 0.38228125\n",
      "Iteration 7, training error = 0.37253125\n",
      "Iteration 8, training error = 0.37549999999999994\n",
      "Iteration 9, training error = 0.37253125\n",
      "Iteration 10, training error = 0.37253125\n",
      "Iteration 11, training error = 0.37253125\n",
      "Iteration 12, training error = 0.37150000000000005\n",
      "Iteration 13, training error = 0.37253125\n",
      "Iteration 14, training error = 0.37150000000000005\n",
      "Iteration 15, training error = 0.37150000000000005\n",
      "Iteration 16, training error = 0.37150000000000005\n",
      "Iteration 17, training error = 0.37150000000000005\n",
      "Iteration 18, training error = 0.37146875\n",
      "Iteration 19, training error = 0.37150000000000005\n",
      "Iteration 20, training error = 0.37146875\n",
      "Iteration 21, training error = 0.37209375\n",
      "Iteration 22, training error = 0.37146875\n",
      "Iteration 23, training error = 0.37212500000000004\n",
      "Iteration 24, training error = 0.37150000000000005\n",
      "Iteration 25, training error = 0.37150000000000005\n",
      "Iteration 26, training error = 0.37212500000000004\n",
      "Iteration 27, training error = 0.37150000000000005\n",
      "Iteration 28, training error = 0.37131250000000005\n",
      "Iteration 29, training error = 0.37121875000000004\n",
      "Iteration 30, training error = 0.37124999999999997\n"
     ]
    }
   ],
   "source": [
    "error_all = []\n",
    "for n in xrange(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], train_data)\n",
    "    error = 1.0 - accuracy_score(train_data[target], predictions)\n",
    "    error_all.append(error)\n",
    "    print \"Iteration %s, training error = %s\" % (n, error_all[n-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing training error vs number of iterations\n",
    "\n",
    "We have provided you with a simple code snippet that plots classification error with the number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcMAAAFNCAYAAAB8PAR2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRKTTUzIUEZFOT8lQREQ6vf8PTy12M9uy+WkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b4112d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on the test data\n",
    "\n",
    "Performing well on the training data is cheating, so lets make sure it works on the `test_data` as well. Here, we will compute the classification error on the `test_data` at the end of each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, test error = 0.41037500000000005\n",
      "Iteration 2, test error = 0.43174999999999997\n",
      "Iteration 3, test error = 0.390875\n",
      "Iteration 4, test error = 0.390875\n",
      "Iteration 5, test error = 0.37825\n",
      "Iteration 6, test error = 0.382625\n",
      "Iteration 7, test error = 0.37175\n",
      "Iteration 8, test error = 0.37612500000000004\n",
      "Iteration 9, test error = 0.37175\n",
      "Iteration 10, test error = 0.37175\n",
      "Iteration 11, test error = 0.37175\n",
      "Iteration 12, test error = 0.369375\n",
      "Iteration 13, test error = 0.369375\n",
      "Iteration 14, test error = 0.369375\n",
      "Iteration 15, test error = 0.369375\n",
      "Iteration 16, test error = 0.369375\n",
      "Iteration 17, test error = 0.369375\n",
      "Iteration 18, test error = 0.371\n",
      "Iteration 19, test error = 0.369375\n",
      "Iteration 20, test error = 0.371\n",
      "Iteration 21, test error = 0.36924999999999997\n",
      "Iteration 22, test error = 0.371\n",
      "Iteration 23, test error = 0.367625\n",
      "Iteration 24, test error = 0.369375\n",
      "Iteration 25, test error = 0.369375\n",
      "Iteration 26, test error = 0.367625\n",
      "Iteration 27, test error = 0.369375\n",
      "Iteration 28, test error = 0.36950000000000005\n",
      "Iteration 29, test error = 0.369\n",
      "Iteration 30, test error = 0.369\n"
     ]
    }
   ],
   "source": [
    "test_error_all = []\n",
    "for n in xrange(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], test_data)\n",
    "    error = 1.0 - accuracy_score(test_data[target], predictions)\n",
    "    test_error_all.append(error)\n",
    "    print \"Iteration %s, test error = %s\" % (n, test_error_all[n-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize both the training and test errors\n",
    "\n",
    "Now, let us plot the training & test error with the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAFTCAYAAAAKvWRNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b21f1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.plot(range(1,31), test_error_all, '-', linewidth=4.0, label='Test error')\n",
    "\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** ii: From this plot (with 30 trees), is there massive overfitting as the # of iterations increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
