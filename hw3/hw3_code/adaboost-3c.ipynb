{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting a decision stump\n",
    "\n",
    "The goal of this notebook is to implement your own boosting module.\n",
    "\n",
    "* Go through an implementation of decision trees.\n",
    "* Implement Adaboost ensembling.\n",
    "* Use your implementation of Adaboost to train a boosted decision stump ensemble.\n",
    "* Evaluate the effect of boosting (adding more decision stumps) on performance of the model.\n",
    "* Explore the robustness of Adaboost to overfitting.\n",
    "\n",
    "*This file is adapted from course material by Carlos Guestrin and Emily Fox.*\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## please make sure that the packages are updated to the newest version. \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a subset of the [LendingClub](https://www.kaggle.com/wendykan/lending-club-loan-data) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = pd.read_csv('loan_small.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recoding the target column\n",
    "\n",
    "We re-assign the target to have +1 as a safe (good) loan, and -1 as a risky (bad) loan. In the next cell, the features are also briefly explained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "\n",
    "loans['safe_loans'] = loans['loan_status'].apply(lambda x : +1 if x=='Fully Paid' else -1)\n",
    "\n",
    "## please update pandas to the newest version in order to execute the following line\n",
    "loans.drop(columns=['loan_status'], inplace=True)\n",
    "\n",
    "target = 'safe_loans' # this variable will be used later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform categorical data into binary features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will work with **binary decision trees**. Since all of our features are currently categorical features, we want to turn them into binary features using 1-hot encoding. \n",
    "\n",
    "We can do so with the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = pd.get_dummies(loans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the feature columns look like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['term_ 36 months',\n",
       " 'term_ 60 months',\n",
       " 'grade_A',\n",
       " 'grade_B',\n",
       " 'grade_C',\n",
       " 'grade_D',\n",
       " 'grade_E',\n",
       " 'grade_F',\n",
       " 'grade_G',\n",
       " 'home_ownership_MORTGAGE',\n",
       " 'home_ownership_NONE',\n",
       " 'home_ownership_OTHER',\n",
       " 'home_ownership_OWN',\n",
       " 'home_ownership_RENT',\n",
       " 'emp_length_1 year',\n",
       " 'emp_length_10+ years',\n",
       " 'emp_length_2 years',\n",
       " 'emp_length_3 years',\n",
       " 'emp_length_4 years',\n",
       " 'emp_length_5 years',\n",
       " 'emp_length_6 years',\n",
       " 'emp_length_7 years',\n",
       " 'emp_length_8 years',\n",
       " 'emp_length_9 years',\n",
       " 'emp_length_< 1 year']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = list(loans.columns)\n",
    "features.remove('safe_loans')  # Remove the response variable\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split\n",
    "\n",
    "We split the data into training and test sets with 80% of the data in the training set and 20% of the data in the test set. We use `seed=1` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(loans, test_size = 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data weights change as we build an AdaBoost model, we need to first code a decision tree that supports weighting of individual data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted error definition\n",
    "\n",
    "Consider a model with $N$ data points with:\n",
    "* Predictions $\\hat{y}_1 ... \\hat{y}_n$ \n",
    "* Target $y_1 ... y_n$ \n",
    "* Data point weights $\\alpha_1 ... \\alpha_n$.\n",
    "\n",
    "Then the **weighted error** is defined by:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$\n",
    "where $1[y_i \\neq \\hat{y_i}]$ is an indicator function that is set to $1$ if $y_i \\neq \\hat{y_i}$.\n",
    "\n",
    "\n",
    "### Write a function to compute weight of mistakes\n",
    "\n",
    "Write a function that calculates the weight of mistakes for making the \"weighted-majority\" predictions for a dataset. The function accepts two inputs:\n",
    "* `labels_in_node`: Targets $y_1 ... y_n$ \n",
    "* `data_weights`: Data point weights $\\alpha_1 ... \\alpha_n$\n",
    "\n",
    "We are interested in computing the (total) weight of mistakes, i.e.\n",
    "$$\n",
    "\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}].\n",
    "$$\n",
    "This quantity is analogous to the number of mistakes, except that each mistake now carries different weight. It is related to the weighted error in the following way:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$\n",
    "\n",
    "The function **intermediate_node_weighted_mistakes** should first compute two weights: \n",
    " * $\\mathrm{WM}_{-1}$: weight of mistakes when all predictions are $\\hat{y}_i = -1$ i.e $\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{-1}$)\n",
    " * $\\mathrm{WM}_{+1}$: weight of mistakes when all predictions are $\\hat{y}_i = +1$ i.e $\\mbox{WM}(\\mathbf{\\alpha}, \\mathbf{+1}$)\n",
    " \n",
    " where $\\mathbf{-1}$ and $\\mathbf{+1}$ are vectors where all values are -1 and +1 respectively.\n",
    " \n",
    "After computing $\\mathrm{WM}_{-1}$ and $\\mathrm{WM}_{+1}$, the function **intermediate_node_weighted_mistakes** should return the lower of the two weights of mistakes, along with the class associated with that weight. We have provided a skeleton for you with `YOUR CODE HERE` to be filled in several places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_node_weighted_mistakes(labels_in_node, data_weights):\n",
    "    # Sum the weights of all entries with label +1\n",
    "    total_weight_positive = sum(data_weights[labels_in_node == +1])\n",
    "    \n",
    "    # Weight of mistakes for predicting all -1's is equal to the sum above\n",
    "    ### YOUR CODE HERE\n",
    "    wm_negative = total_weight_positive\n",
    "    \n",
    "    # Sum the weights of all entries with label -1\n",
    "    ### YOUR CODE HERE\n",
    "    total_weight_negative = sum(data_weights[labels_in_node == -1])\n",
    "    \n",
    "    # Weight of mistakes for predicting all +1's is equal to the sum above\n",
    "    ### YOUR CODE HERE\n",
    "    wm_positive = total_weight_negative\n",
    "    \n",
    "    # Return the tuple (weight, class_label) representing the lower of the two weights\n",
    "    #    class_label should be an integer of value +1 or -1.\n",
    "    # If the two weights are identical, return (weighted_mistakes_all_positive,+1)\n",
    "    ### YOUR CODE HERE\n",
    "    if wm_negative < wm_positive:\n",
    "        return (wm_negative, -1)\n",
    "    else:\n",
    "        return (wm_positive, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Test your **intermediate_node_weighted_mistakes** function, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_labels = pd.Series([-1, -1, 1, 1, 1])\n",
    "example_data_weights = pd.Series([1., 2., .5, 1., 1.])\n",
    "if intermediate_node_weighted_mistakes(example_labels, example_data_weights) == (2.5, -1):\n",
    "    print('Test passed!')\n",
    "else:\n",
    "    print('Test failed... try again!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# all data points}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to pick best feature to split on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to pick the best feature to split on.\n",
    "\n",
    "The **best_splitting_feature** function takes the data, the festures, the targetm and the data weights as input and returns the best feature to split on.\n",
    "  \n",
    "Complete the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the data is identical in each feature, this function should return None\n",
    "\n",
    "def best_splitting_feature(data, features, target, data_weights):\n",
    "    \n",
    "    # These variables will keep track of the best feature and the corresponding error\n",
    "    best_feature = None\n",
    "    best_error = float('+inf') \n",
    "    num_points = float(len(data))\n",
    "\n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # The left split will have all data points where the feature value is 0\n",
    "        # The right split will have all data points where the feature value is 1\n",
    "        left_split = data[data[feature] == 0]\n",
    "        right_split = data[data[feature] == 1]\n",
    "        \n",
    "        # Apply the same filtering to data_weights to create left_data_weights, right_data_weights\n",
    "        ## YOUR CODE HERE\n",
    "        left_data_weights = data_weights[data[feature] == 0]\n",
    "        right_data_weights = data_weights[data[feature] == 1]\n",
    "                    \n",
    "        # Calculate the weight of mistakes for left and right sides\n",
    "        ## YOUR CODE HERE\n",
    "        left_wm, left_label = intermediate_node_weighted_mistakes(left_split[target], left_data_weights)\n",
    "        right_wm, right_label = intermediate_node_weighted_mistakes(right_split[target], right_data_weights)\n",
    "        \n",
    "        # Compute weighted error by computing\n",
    "        #  ( [weight of mistakes (left)] + [weight of mistakes (right)] ) / [total weight of all data points]\n",
    "        ## YOUR CODE HERE\n",
    "        error = (left_wm + right_wm) / sum(data_weights)\n",
    "        \n",
    "        # If this is the best error we have found so far, store the feature and the error\n",
    "        if error < best_error:\n",
    "            best_feature = feature\n",
    "            best_error = error\n",
    "    \n",
    "    # Return the best feature we found\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Now, we have another checkpoint to make sure you are on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_data_weights = np.array(len(train_data)* [1.5])\n",
    "if best_splitting_feature(train_data, features, target, example_data_weights) == 'term_ 36 months':\n",
    "    print('Test passed!')\n",
    "else:\n",
    "    print('Test failed... try again!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside**. Relationship between weighted error and weight of mistakes:\n",
    "\n",
    "By definition, the weighted error is the weight of mistakes divided by the weight of all data points, so\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]}{\\sum_{i=1}^{n} \\alpha_i} = \\frac{\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\sum_{i=1}^{n} \\alpha_i}.\n",
    "$$\n",
    "\n",
    "In the code above, we obtain $\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ from the two weights of mistakes from both sides, $\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}})$ and $\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})$. First, notice that the overall weight of mistakes $\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ can be broken into two weights of mistakes over either side of the split:\n",
    "$$\n",
    "\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})\n",
    "= \\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\n",
    "= \\sum_{\\mathrm{left}} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\n",
    " + \\sum_{\\mathrm{right}} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\\\\\n",
    "= \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}}) + \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})\n",
    "$$\n",
    "We then divide through by the total weight of all data points to obtain $\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})\n",
    "= \\frac{\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}}) + \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the tree\n",
    "\n",
    "With the above functions implemented correctly, we are now ready to build our decision tree. A decision tree will be represented as a dictionary which contains the following keys:\n",
    "\n",
    "    { \n",
    "       'is_leaf'            : True/False.\n",
    "       'prediction'         : Prediction at the leaf node.\n",
    "       'left'               : (dictionary corresponding to the left tree).\n",
    "       'right'              : (dictionary corresponding to the right tree).\n",
    "       'features_remaining' : List of features that are posible splits.\n",
    "    }\n",
    "    \n",
    "Let us start with a function that creates a leaf node given a set of target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_leaf(target_values, data_weights):\n",
    "    \n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'is_leaf': True}\n",
    "    \n",
    "    # Computed weight of mistakes.\n",
    "    weighted_error, best_class = intermediate_node_weighted_mistakes(target_values, data_weights)\n",
    "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
    "    ## YOUR CODE HERE\n",
    "    leaf['prediction'] = best_class\n",
    "    \n",
    "    return leaf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a function that learns a weighted decision tree recursively and implements 3 stopping conditions:\n",
    "1. All data points in a node are from the same class.\n",
    "2. No more features to split on.\n",
    "3. Stop growing the tree when the tree depth reaches **max_depth**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_decision_tree_create(data, features, target, data_weights, current_depth = 1, max_depth = 10):\n",
    "    remaining_features = features[:] # Make a copy of the features.\n",
    "    target_values = data[target]\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"Subtree, depth = {} ({} data points).\".format(current_depth, len(target_values)))\n",
    "    \n",
    "    # Stopping condition 1. Error is 0.\n",
    "    if intermediate_node_weighted_mistakes(target_values, data_weights)[0] <= 1e-15:\n",
    "        print(\"Stopping condition 1 reached.\")\n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # Stopping condition 2. No more features.\n",
    "    if remaining_features == []:\n",
    "        print(\"Stopping condition 2 reached.\")              \n",
    "        return create_leaf(target_values, data_weights)    \n",
    "    \n",
    "    # Additional stopping condition (limit tree depth)\n",
    "    if current_depth > max_depth:\n",
    "        print(\"Reached maximum depth. Stopping for now.\")\n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # If all the datapoints are the same, splitting_feature will be None. Create a leaf\n",
    "    splitting_feature = best_splitting_feature(data, features, target, data_weights)\n",
    "    remaining_features.remove(splitting_feature)\n",
    "        \n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    right_split = data[data[splitting_feature] == 1]\n",
    "    \n",
    "    left_data_weights = data_weights[data[splitting_feature] == 0]\n",
    "    right_data_weights = data_weights[data[splitting_feature] == 1]\n",
    "    \n",
    "    print(\"Split on feature {:s}. ({}, {})\".format(splitting_feature, len(left_split), len(right_split)))\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if len(left_split) == len(data):\n",
    "        print(\"Creating leaf node.\")\n",
    "        return create_leaf(left_split[target], data_weights)\n",
    "    if len(right_split) == len(data):\n",
    "        print(\"Creating leaf node.\")\n",
    "        return create_leaf(right_split[target], data_weights)\n",
    "    \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    ## YOUR CODE HERE\n",
    "    left_tree = weighted_decision_tree_create(left_split, remaining_features, target, left_data_weights, current_depth+1, max_depth)\n",
    "    right_tree = weighted_decision_tree_create(right_split, remaining_features, target, right_data_weights, current_depth+1, max_depth)\n",
    "    \n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a recursive function to count the nodes in your tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following test code to check your implementation. Make sure you get **'Test passed'** before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Split on feature grade_A. (8775, 75)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (8775 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (75 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Split on feature grade_D. (19331, 3819)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (19331 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (3819 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_data_weights = np.array([1.0 for i in range(len(train_data))])\n",
    "small_data_decision_tree = weighted_decision_tree_create(train_data, features, target,\n",
    "                                        example_data_weights, max_depth=2)\n",
    "if count_nodes(small_data_decision_tree) == 7:\n",
    "    print('Test passed!')\n",
    "else:\n",
    "    print('Test failed... try again!')\n",
    "    print('Number of nodes found:', count_nodes(small_data_decision_tree))\n",
    "    print('Number of nodes that should be there: 7' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a quick look at what the trained tree is like. You should get something that looks like the following\n",
    "\n",
    "```\n",
    "{'is_leaf': False,\n",
    "    'left': {'is_leaf': False,\n",
    "        'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
    "        'prediction': None,\n",
    "        'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
    "        'splitting_feature': 'grade_A'\n",
    "     },\n",
    "    'prediction': None,\n",
    "    'right': {'is_leaf': False,\n",
    "        'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
    "        'prediction': None,\n",
    "        'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
    "        'splitting_feature': 'grade_D'\n",
    "     },\n",
    "     'splitting_feature': 'term. 36 months'\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_leaf': False,\n",
       " 'left': {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade_A'},\n",
       " 'prediction': None,\n",
       " 'right': {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade_D'},\n",
       " 'splitting_feature': 'term_ 36 months'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with a weighted decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give you a function that classifies one data point. It can also return the probability if you want to play around with that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):   \n",
    "    # If the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate: \n",
    "            print(\"At leaf, predicting {}\".format(tree['prediction']))\n",
    "        return tree['prediction'] \n",
    "    else:\n",
    "        # Split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate: \n",
    "            print(\"Split on {} = {}\".format(tree['splitting_feature'], split_feature_value))\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the tree\n",
    "\n",
    "Now, we will write a function to evaluate a decision tree by computing the classification error of the tree on the given dataset.\n",
    "\n",
    "Again, recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# all data points}}\n",
    "$$\n",
    "\n",
    "The function called **evaluate_classification_error** takes in as input:\n",
    "1. `tree` (as described above)\n",
    "2. `data` (a dataframe)\n",
    "\n",
    "The function does not change because of adding data point weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_error(tree, data):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    # YOUR CODE HERE\n",
    "    prediction = []\n",
    "    for index, row in data.iterrows():\n",
    "        prediction.append(classify(tree, row))\n",
    "    \n",
    "    # Once you've made the predictions, calculate the classification error\n",
    "    return (prediction != data[target]).sum() / float(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.390875"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Training a weighted decision tree\n",
    "\n",
    "To build intuition on how weighted data points affect the tree being built, consider the following:\n",
    "\n",
    "Suppose we only care about making good predictions for the **first 10 and last 10 items** in `train_data`, we assign weights:\n",
    "* 1 to the last 10 items \n",
    "* 1 to the first 10 items \n",
    "* and 0 to the rest. \n",
    "\n",
    "Let us fit a weighted decision tree with `max_depth = 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature emp_length_10+ years. (22413, 9587)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (22413 data points).\n",
      "Split on feature grade_A. (19673, 2740)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (19673 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (2740 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9587 data points).\n",
      "Stopping condition 1 reached.\n"
     ]
    }
   ],
   "source": [
    "# Assign weights\n",
    "example_data_weights = np.array([1.] * 10 + [0.]*(len(train_data) - 20) + [1.] * 10)\n",
    "\n",
    "# Train a weighted decision tree model.\n",
    "small_data_decision_tree_subset_20 = weighted_decision_tree_create(train_data, features, target,\n",
    "                         example_data_weights, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will compute the classification error on the `subset_20`, i.e. the subset of data points whose weight is 1 (namely the first and last 10 data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_20 = train_data.head(10).append(train_data.tail(10))\n",
    "evaluate_classification_error(small_data_decision_tree_subset_20, subset_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us compare the classification error of the model `small_data_decision_tree_subset_20` on the entire test set `train_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.445625"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree_subset_20, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model `small_data_decision_tree_subset_20` performs **a lot** better on `subset_20` than on `train_data`.\n",
    "\n",
    "So, what does this mean?\n",
    "* The points with higher weights are the ones that are more important during the training process of the weighted decision tree.\n",
    "* The points with zero weights are basically ignored during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing your own Adaboost (on decision stumps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a weighted decision tree working, it takes only a bit of work to implement Adaboost. For the sake of simplicity, let us stick with **decision tree stumps** by training trees with **`max_depth=1`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the lecture notes the procedure for Adaboost:\n",
    "\n",
    "1\\. Start with unweighted data with $\\alpha_j = 1$\n",
    "\n",
    "2\\. For t = 1,...T:\n",
    "  * Learn $f_t(x)$ with data weights $\\alpha_j$\n",
    "  * Compute coefficient $\\hat{w}_t$:\n",
    "     $$\\hat{w}_t = \\frac{1}{2}\\ln{\\left(\\frac{1- \\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}\\right)}$$\n",
    "  * Re-compute weights $\\alpha_j$:\n",
    "     $$\\alpha_j \\gets \\begin{cases}\n",
    "     \\alpha_j \\exp{(-\\hat{w}_t)} & \\text{ if }f_t(x_j) = y_j\\\\\n",
    "     \\alpha_j \\exp{(\\hat{w}_t)} & \\text{ if }f_t(x_j) \\neq y_j\n",
    "     \\end{cases}$$\n",
    "  * Normalize weights $\\alpha_j$:\n",
    "      $$\\alpha_j \\gets \\frac{\\alpha_j}{\\sum_{i=1}^{N}{\\alpha_i}} $$\n",
    "  \n",
    "Complete the skeleton for the following code to implement **adaboost_with_tree_stumps**. Fill in the places with `YOUR CODE HERE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "def adaboost_with_tree_stumps(data, features, target, num_tree_stumps):\n",
    "    # start with unweighted data (uniformly weighted)\n",
    "    alpha =  np.array([1.]*len(data))\n",
    "    weights = []\n",
    "    tree_stumps = []\n",
    "    target_values = data[target]\n",
    "    \n",
    "    for t in range(num_tree_stumps):\n",
    "        print('=====================================================')\n",
    "        print('Adaboost Iteration {}'.format(t))\n",
    "        print('=====================================================')      \n",
    "        # Learn a weighted decision tree stump. Use max_depth=1\n",
    "        # YOUR CODE HERE\n",
    "        tree = weighted_decision_tree_create(data, features, target, alpha, max_depth=1)\n",
    "        tree_stumps.append(tree)\n",
    "        \n",
    "        # Make predictions\n",
    "        ## YOUR CODE HERE\n",
    "        predictions = []\n",
    "        for _, row in data.iterrows():\n",
    "            predictions.append(classify(tree, row))\n",
    "        \n",
    "        # Produce a Boolean array indicating whether\n",
    "        # each data point was correctly classified\n",
    "        is_correct = predictions == target_values\n",
    "        is_wrong   = predictions != target_values\n",
    "        \n",
    "        # Compute weighted error\n",
    "        ## YOUR CODE HERE\n",
    "        weighted_mistake = np.sum(alpha[is_wrong.data]) # get numpy array out of the pandas dataframe\n",
    "        weighted_error = weighted_mistake / np.sum(alpha)\n",
    "        \n",
    "        # Compute model coefficient using weighted error\n",
    "        ## YOUR CODE HERE\n",
    "        weight = 0.5 * log((1 - weighted_error) / weighted_error)\n",
    "        weights.append(weight)\n",
    "        \n",
    "        # Adjust weights on data point\n",
    "        ## YOUR CODE HERE\n",
    "        adjustment = is_correct.apply(lambda is_correct : exp(-weight) if is_correct else exp(weight))\n",
    "\n",
    "        # Scale alpha by multiplying by adjustment \n",
    "        # Then normalize data points weights\n",
    "        ## YOUR CODE HERE \n",
    "        alpha = alpha * np.array(adjustment)\n",
    "        norm = np.sum(alpha)\n",
    "        alpha = alpha / norm\n",
    "        print(alpha.shape)\n",
    "    \n",
    "    return weights, tree_stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking your Adaboost code\n",
    "\n",
    "Train an ensemble of **two** tree stumps and see which features those stumps split on. We will run the algorithm with the following parameters:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, target, num_tree_stumps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stump(tree):\n",
    "    split_name = tree['splitting_feature'] # split_name is something like 'term. 36 months'\n",
    "    if split_name is None:\n",
    "        print (\"(leaf, label: {})\".format(tree['prediction']))\n",
    "        return None\n",
    "    split_feature, split_value = split_name.split('_')\n",
    "    print('                       root')\n",
    "    print('         |---------------|----------------|')\n",
    "    print('         |                                |')\n",
    "    print('         |                                |')\n",
    "    print('         |                                |')\n",
    "    print('  [{0} == 0]{1}[{0} == 1]    '.format(split_name, ' '*(27-len(split_name))))\n",
    "    print('         |                                |')\n",
    "    print('         |                                |')\n",
    "    print('         |                                |')\n",
    "    print('    ({})                 ({})'.format\n",
    "           (('leaf, label: ' + str(tree['left']['prediction']) if tree['left']['is_leaf'] else 'subtree'),\n",
    "           ('leaf, label: ' + str(tree['right']['prediction']) if tree['right']['is_leaf'] else 'subtree')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the first stump looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [term_ 36 months == 0]            [term_ 36 months == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(tree_stumps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the next stump looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade_A == 0]                    [grade_A == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(tree_stumps[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.17198848113764034, 0.17728780637267771]\n"
     ]
    }
   ],
   "source": [
    "print(stump_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your Adaboost is correctly implemented, the following things should be true:\n",
    "\n",
    "* `tree_stumps[0]` should split on **term. 36 months** with the prediction -1 on the left and +1 on the right.\n",
    "* `tree_stumps[1]` should split on **grade.A** with the prediction -1 on the left and +1 on the right.\n",
    "* Weights should be approximately `[0.158, 0.177]` \n",
    "\n",
    "**Reminders**\n",
    "- Stump weights ($\\mathbf{\\hat{w}}$) and data point weights ($\\mathbf{\\alpha}$) are two different concepts.\n",
    "- Stump weights ($\\mathbf{\\hat{w}}$) tell you how important each stump is while making predictions with the entire boosted ensemble.\n",
    "- Data point weights ($\\mathbf{\\alpha}$) tell you how important each data point is while training a decision stump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a boosted ensemble of 10 stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train an ensemble of 10 decision tree stumps with Adaboost. We run the **adaboost_with_tree_stumps** function with the following parameters:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_D. (26027, 5973)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26027 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5973 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_B. (23457, 8543)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23457 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8543 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (16870, 15130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16870 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (15130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, \n",
    "                                target, num_tree_stumps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "Recall from the lecture that in order to make predictions, we use the following formula:\n",
    "$$\n",
    "\\hat{y} = sign\\left(\\sum_{t=1}^T \\hat{w}_t f_t(x)\\right)\n",
    "$$\n",
    "\n",
    "We need to do the following things:\n",
    "- Compute the predictions $f_t(x)$ using the $t$-th decision tree\n",
    "- Compute $\\hat{w}_t f_t(x)$ by multiplying the `stump_weights` with the predictions $f_t(x)$ from the decision trees\n",
    "- Sum the weighted predictions over each stump in the ensemble.\n",
    "\n",
    "Complete the following skeleton for making predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_adaboost(stump_weights, tree_stumps, data):\n",
    "    scores = np.array([0.]*len(data))\n",
    "    \n",
    "    for i, tree_stump in enumerate(tree_stumps):\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x), axis = 1)\n",
    "        \n",
    "        # Accumulate predictions on scaores array\n",
    "        # YOUR CODE HERE\n",
    "        weighted_predictions = stump_weights[i] * predictions\n",
    "        scores = scores + weighted_predictions\n",
    "        \n",
    "    return scores.apply(lambda score : +1 if score > 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 10-component ensemble = 0.62825\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_adaboost(stump_weights, tree_stumps, test_data)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_data[target], predictions)\n",
    "print('Accuracy of 10-component ensemble = {}'.format(accuracy)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us take a quick look what the `stump_weights` look like at the end of each iteration of the 10-stump ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17198848113764034,\n",
       " 0.17728780637267771,\n",
       " 0.10308067696993935,\n",
       " 0.08686702058341694,\n",
       " 0.07220085937787603,\n",
       " 0.07438562925255676,\n",
       " 0.05834552873231902,\n",
       " 0.045454870264690556,\n",
       " 0.03194548460003601,\n",
       " 0.02330529243221465]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stump_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** i: Are the weights monotonically decreasing, monotonically increasing, or neither?\n",
    "\n",
    "**Reminder**: Stump weights ($\\mathbf{\\hat{w}}$) tell you how important each stump is while making predictions with the entire boosted ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights are monotonically decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance plots\n",
    "\n",
    "In this section, we will try to reproduce some performance plots.\n",
    "\n",
    "### How does accuracy change with adding stumps to the ensemble?\n",
    "\n",
    "We will now train an ensemble with:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 30`\n",
    "\n",
    "Once we are done with this, we will then do the following:\n",
    "* Compute the classification error at the end of each iteration.\n",
    "* Plot a curve of classification error vs iteration.\n",
    "\n",
    "First, lets train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_D. (26027, 5973)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26027 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5973 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_B. (23457, 8543)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23457 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8543 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (16870, 15130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16870 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (15130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 10\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 11\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 12\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature emp_length_10+ years. (22413, 9587)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (22413 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9587 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 13\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_B. (23457, 8543)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23457 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8543 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 14\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 15\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_D. (26027, 5973)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26027 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5973 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 16\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 17\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 18\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_E. (28766, 3234)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28766 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3234 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 19\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_C. (23388, 8612)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23388 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8612 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 20\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (16870, 15130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16870 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (15130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 21\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature term_ 36 months. (8850, 23150)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8850 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23150 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 22\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_F. (30624, 1376)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30624 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1376 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 23\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_B. (23457, 8543)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23457 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (8543 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 24\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature emp_length_2 years. (29104, 2896)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (29104 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (2896 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 25\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_G. (31657, 343)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (31657 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (343 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 26\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_A. (28081, 3919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28081 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 27\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_G. (31657, 343)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (31657 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (343 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 28\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature home_ownership_OWN. (29204, 2796)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (29204 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (2796 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n",
      "=====================================================\n",
      "Adaboost Iteration 29\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (32000 data points).\n",
      "Split on feature grade_G. (31657, 343)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (31657 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (343 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "(32000,)\n"
     ]
    }
   ],
   "source": [
    "# this may take a while... \n",
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, \n",
    "                                 features, target, num_tree_stumps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing training error at the end of each iteration\n",
    "\n",
    "Now, we will compute the classification error on the **train_data** and see how it is reduced as trees are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, training error = 0.41484374999999996\n",
      "Iteration 2, training error = 0.43281250000000004\n",
      "Iteration 3, training error = 0.39059374999999996\n",
      "Iteration 4, training error = 0.39059374999999996\n",
      "Iteration 5, training error = 0.37931250000000005\n",
      "Iteration 6, training error = 0.38228125\n",
      "Iteration 7, training error = 0.37253125\n",
      "Iteration 8, training error = 0.37549999999999994\n",
      "Iteration 9, training error = 0.37253125\n",
      "Iteration 10, training error = 0.37253125\n",
      "Iteration 11, training error = 0.37253125\n",
      "Iteration 12, training error = 0.37150000000000005\n",
      "Iteration 13, training error = 0.37253125\n",
      "Iteration 14, training error = 0.37150000000000005\n",
      "Iteration 15, training error = 0.37150000000000005\n",
      "Iteration 16, training error = 0.37150000000000005\n",
      "Iteration 17, training error = 0.37150000000000005\n",
      "Iteration 18, training error = 0.37146875\n",
      "Iteration 19, training error = 0.37150000000000005\n",
      "Iteration 20, training error = 0.37146875\n",
      "Iteration 21, training error = 0.37209375\n",
      "Iteration 22, training error = 0.37146875\n",
      "Iteration 23, training error = 0.37212500000000004\n",
      "Iteration 24, training error = 0.37150000000000005\n",
      "Iteration 25, training error = 0.37150000000000005\n",
      "Iteration 26, training error = 0.37212500000000004\n",
      "Iteration 27, training error = 0.37150000000000005\n",
      "Iteration 28, training error = 0.37131250000000005\n",
      "Iteration 29, training error = 0.37121875000000004\n",
      "Iteration 30, training error = 0.37124999999999997\n"
     ]
    }
   ],
   "source": [
    "error_all = []\n",
    "for n in range(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], train_data)\n",
    "    error = 1.0 - accuracy_score(train_data[target], predictions)\n",
    "    error_all.append(error)\n",
    "    print(\"Iteration {}, training error = {}\".format(n, error_all[n-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing training error vs number of iterations\n",
    "\n",
    "We have provided you with a simple code snippet that plots classification error with the number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1148af550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on the test data\n",
    "\n",
    "Performing well on the training data is cheating, so lets make sure it works on the `test_data` as well. Here, we will compute the classification error on the `test_data` at the end of each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, test error = 0.41037500000000005\n",
      "Iteration 2, test error = 0.43174999999999997\n",
      "Iteration 3, test error = 0.390875\n",
      "Iteration 4, test error = 0.390875\n",
      "Iteration 5, test error = 0.37825\n",
      "Iteration 6, test error = 0.382625\n",
      "Iteration 7, test error = 0.37175\n",
      "Iteration 8, test error = 0.37612500000000004\n",
      "Iteration 9, test error = 0.37175\n",
      "Iteration 10, test error = 0.37175\n",
      "Iteration 11, test error = 0.37175\n",
      "Iteration 12, test error = 0.369375\n",
      "Iteration 13, test error = 0.369375\n",
      "Iteration 14, test error = 0.369375\n",
      "Iteration 15, test error = 0.369375\n",
      "Iteration 16, test error = 0.369375\n",
      "Iteration 17, test error = 0.369375\n",
      "Iteration 18, test error = 0.371\n",
      "Iteration 19, test error = 0.369375\n",
      "Iteration 20, test error = 0.371\n",
      "Iteration 21, test error = 0.36924999999999997\n",
      "Iteration 22, test error = 0.371\n",
      "Iteration 23, test error = 0.367625\n",
      "Iteration 24, test error = 0.369375\n",
      "Iteration 25, test error = 0.369375\n",
      "Iteration 26, test error = 0.367625\n",
      "Iteration 27, test error = 0.369375\n",
      "Iteration 28, test error = 0.36950000000000005\n",
      "Iteration 29, test error = 0.369\n",
      "Iteration 30, test error = 0.369\n"
     ]
    }
   ],
   "source": [
    "test_error_all = []\n",
    "for n in range(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], test_data)\n",
    "    error = 1.0 - accuracy_score(test_data[target], predictions)\n",
    "    test_error_all.append(error)\n",
    "    print(\"Iteration {}, test error = {}\".format(n, test_error_all[n-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize both the training and test errors\n",
    "\n",
    "Now, let us plot the training & test error with the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1117a99e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.plot(range(1,31), test_error_all, '-', linewidth=4.0, label='Test error')\n",
    "\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** ii: From this plot (with 30 trees), is there massive overfitting as the # of iterations increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As number of trees/iterations increases, the testing error is getting smaller than training error, so there is no massive overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
