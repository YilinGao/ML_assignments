\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
%\usepackage{authblk}
\usepackage{graphicx}
\usepackage{url}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{mathrsfs}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage[skip=2pt,it]{caption}
\usepackage{subcaption}
\usepackage{amsmath}

\newcommand{\Amat}[0]{{{\bf A}}}
\newcommand{\Bmat}{{\bf B}}
\newcommand{\Cmat}{{\bf C}}
\newcommand{\Dmat}{{\bf D}}
\newcommand{\Emat}[0]{{{\bf E}}}
\newcommand{\Fmat}[0]{{{\bf F}}\xspace}
\newcommand{\Gmat}{{\bf G}}
\newcommand{\Hmat}{{\bf H}}
\newcommand{\Imat}{{\bf I}}
\newcommand{\Jmat}[0]{{{\bf J}}\xspace}
\newcommand{\Kmat}[0]{{{\bf K}}\xspace}
\newcommand{\Lmat}[0]{{{\bf L}}}
%\newcommand{\Mmat}[0]{{{\bf M}}\xspace}
\newcommand{\Mmat}{{\bf M}}
\newcommand{\Nmat}[0]{{{\bf N}}\xspace}
\newcommand{\Omat}[0]{{{\bf O}}}
\newcommand{\Pmat}{{\bf P}}
\newcommand{\Qmat}[0]{{{\bf Q}}\xspace}
\newcommand{\Rmat}[0]{{{\bf R}}}
\newcommand{\Smat}[0]{{{\bf S}}}
\newcommand{\Tmat}[0]{{{\bf T}}}
\newcommand{\Umat}[0]{{{\bf U}}}
\newcommand{\Vmat}[0]{{{\bf V}}}
\newcommand{\Wmat}[0]{{{\bf W}}}
\newcommand{\Xmat}[0]{{{\bf X}}}
\newcommand{\Ymat}{{\bf Y}}
%\newcommand{\Ymat}[0]{{{\bf Z}}}
\newcommand{\Zmat}{{\bf Z}}

\newcommand{\av}[0]{{\boldsymbol{a}}}
\newcommand{\bv}[0]{{\boldsymbol{b}}}
\newcommand{\cv}[0]{{\boldsymbol{c}}}
\newcommand{\dv}{\boldsymbol{d}}
\newcommand{\ev}[0]{{\boldsymbol{e}}\xspace}
\newcommand{\fv}[0]{{\boldsymbol{f}}}
\newcommand{\gv}[0]{{\boldsymbol{g}}\xspace}
\newcommand{\hv}[0]{{\boldsymbol{h}}}
\newcommand{\iv}[0]{{\boldsymbol{i}}}
\newcommand{\jv}[0]{{\boldsymbol{j}}\xspace}
\newcommand{\kv}[0]{{\boldsymbol{k}}\xspace}
\newcommand{\lv}[0]{{\boldsymbol{l}}}
\newcommand{\mv}[0]{{\boldsymbol{m}}}
\newcommand{\nv}[0]{{\boldsymbol{n}}\xspace}
\newcommand{\ov}[0]{{\boldsymbol{o}}}
\newcommand{\pv}[0]{{\boldsymbol{p}}}
\newcommand{\qv}[0]{{\boldsymbol{q}}\xspace}
\newcommand{\rv}{\boldsymbol{r}}
\newcommand{\sv}[0]{{\boldsymbol{s}}}
\newcommand{\tv}[0]{{\boldsymbol{t}}\xspace}
\newcommand{\uv}{\boldsymbol{u}}
\newcommand{\vv}{\boldsymbol{v}}
\newcommand{\wv}{\boldsymbol{w}}
\newcommand{\xv}{\boldsymbol{x}}
\newcommand{\yv}{\boldsymbol{y}}
\newcommand{\zv}{\boldsymbol{z}}
\newcommand{\cdotv}{\boldsymbol{\cdot}}

\newcommand{\Gammamat}[0]{{\boldsymbol{\Gamma}}\xspace}
\newcommand{\Deltamat}[0]{{\boldsymbol{\Delta}}\xspace}
\newcommand{\Thetamat}{\boldsymbol{\Theta}}
\newcommand{\Betamat}{\boldsymbol{\Beta}}
\newcommand{\Lambdamat}{\boldsymbol{\Lambda}}
\newcommand{\Ximat}[0]{{\boldsymbol{\Xi}}\xspace}
\newcommand{\Pimat}[0]{{\boldsymbol{\Pi}}\xspace}
\newcommand{\Sigmamat}[0]{{\boldsymbol{\Sigma}}}
\newcommand{\Upsilonmat}[0]{{\boldsymbol{\Upsilon}}\xspace}
\newcommand{\Phimat}{\boldsymbol{\Phi}}
\newcommand{\Psimat}{\boldsymbol{\Psi}}
\newcommand{\Omegamat}[0]{{\boldsymbol{\Omega}}}

\newcommand{\alphav}{\boldsymbol{\alpha}}
\newcommand{\betav}[0]{{\boldsymbol{\beta}}}
\newcommand{\gammav}[0]{{\boldsymbol{\gamma}}\xspace}
\newcommand{\deltav}[0]{{\boldsymbol{\delta}}\xspace}
\newcommand{\epsilonv}{\boldsymbol{\epsilon}}
\newcommand{\zetav}{\boldsymbol{\zeta}}
\newcommand{\etav}[0]{{\boldsymbol{\eta}}\xspace}
\newcommand{\ellv}[0]{{\boldsymbol{\ell}}}
\newcommand{\thetav}{\boldsymbol{\theta}}
\newcommand{\iotav}[0]{{\boldsymbol{\iota}}}
\newcommand{\kappav}[0]{{\boldsymbol{\kappa}}\xspace}
\newcommand{\lambdav}[0]{{\boldsymbol{\lambda}}}
\newcommand{\muv}[0]{{\boldsymbol{\mu}}}
\newcommand{\nuv}[0]{{\boldsymbol{\nu}}}
\newcommand{\xiv}[0]{{\boldsymbol{\xi}}\xspace}
\newcommand{\omicronv}[0]{{\boldsymbol{\omicron}}\xspace}
\newcommand{\piv}{\boldsymbol{\pi}}
\newcommand{\rhov}[0]{{\boldsymbol{\rho}}\xspace}
\newcommand{\sigmav}[0]{{\boldsymbol{\sigma}}}
\newcommand{\tauv}[0]{{\boldsymbol{\tau}}\xspace}
\newcommand{\upsilonv}[0]{{\boldsymbol{\upsilon}}\xspace}
\newcommand{\phiv}{\boldsymbol{\phi}}
\newcommand{\chiv}[0]{{\boldsymbol{\chi}}\xspace}
\newcommand{\psiv}{\boldsymbol{\psi}}
\newcommand{\varthetav}{\boldsymbol{\vartheta}}
\newcommand{\omegav}[0]{{\boldsymbol{\omega}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Z}{\mathbb{Z}}

\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\NNcal}{\mathcal{N}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Ical}{\mathcal{I}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Qcal}{\mathcal{Q}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\Jcal}{\mathcal{J}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\Wcal}{\mathcal{W}}
\newcommand{\Ccal}{\mathcal{C}}

\begin{document}

\title{Homework 5}

\author{2018 Spring STA 561}

\maketitle
\section{Hoeffdingâ€™s Inequality (20 pts)}
\paragraph{a. (15 pts)} Chernoff Bounds: Let $X$ be a random variable, for any $t\geq0$
\begin{align*}
	Pr(X\geq\mu_X+t)\leq\min_{\lambda\geq 0}M_{X-\mu_X}(\lambda)e^{-\lambda t},
\end{align*}
where $\mu_X=\mathbb{E}[X]$ is the mean and $M_X(\lambda)=\mathbb{E}[e^{\lambda X}]$ is the moment generating function.

Hoeffding's Lemma: Let $X$ be a bounded random variable with $X\in[a,b]$. Then
\begin{align*}
\mathbb{E}[e^{\lambda(X-\mu_X)}]\leq\exp(\frac{\lambda^2(b-a)^2}{8}), \text{ for all }\lambda\in\mathbb{R}.
\end{align*}
Use Chernoff bounds and Hoeffding's lemma to prove Hoeffding's inequality
\begin{align*}
Pr(\frac{1}{n}\sum_{i=1}^{n}(X_i-\mu_{X_i})\geq t)\leq \exp(-\frac{2nt^2}{(b-a)^2}), \text{ for all } t\geq 0.
\end{align*}
where $X_1,...,X_n$ are independent random variables with $X_i\in[a,b]$ for all $i$.

\paragraph{b. (5 pts)} Hoeffding's inequality is very loose in certain cases. Please give a simple distribution of $X_i$ where the bound can be much sharper than Hoeffding's bound.

\section{VC Dimension (40 pts)}

Given data $(x_i,y_i)_i^n$ drawn from a complicated binary classification function. We have the following two kernel functions $k_1, k_2$, two hypothesis spaces $\Hcal_1,\Hcal_2$, and two estimators $\hat{f}_1,\hat{f}_2$:

\noindent The linear kernel: $k_1(\uv,\vv)=\uv^T\vv$. 

\noindent The second order polynomial kernel: $k_2(\uv,\vv)=(\uv^T\vv+1)^2$.
\begin{align*}
\Hcal_1 &= (f:f(\xv)=Sign[\sum_{i=1}^{N}\alpha_i\xv_i^T\xv])\\
\Hcal_2 &= (f:f(\xv)=Sign[\sum_{i=1}^{N}\alpha_i(\xv_i^T\xv+1)^2])\\
\hat{f}_1 &= \arg\min_{f\in\Hcal_1}\frac{1}{n}\sum_{i=1}^{n}\mathbf{I}(y_i\neq f(\xv_i))\\
\hat{f}_2 &= \arg\min_{f\in\Hcal_2}\frac{1}{n}\sum_{i=1}^{n}\mathbf{I}(y_i\neq f(\xv_i))
\end{align*}
\noindent where $\uv, \vv\in\mathbb{R}^p, \alpha_i\in\mathbb{R}, \xv_i\in\mathbb{R}^p, y_i\in\{0,1\}, N\in\mathbb{Z}_+$.

\paragraph{a. (10 pts)} What is the VC-dimension of $\Hcal_1$ and $\Hcal_2$.
\paragraph{b. (20 pts)} Draw a picture for the approximation and estimation error for $\Hcal_1,\Hcal_2$ and $\hat{f}_1,\hat{f}_2$ and write them down. Explain how the two errors change as n increases. (Hint: you may find the picture and notations in the notes helpful.)
\paragraph{c. (10 pts)} Please find at least one function class F where the VC dimension is not equal to the number of parameters of the function class. This will demonstrate that complexity of a function class is not always measured by the number of parameters. (Hint: If you have trouble you can look it up on the Internet. Hint 2: Prof. Rudin will provide an example of this in the lecture that you can use.)

\section{Ridge Regression (40 pts)}
Given a response vector $\yv\in\mathbb{R}^n$ and a predicator matrix $\Xmat\in\mathbb{R}^{n\times p}$, the ridge regression coefficients are defined as
\begin{align*}
\hat{\betav}^{ridge} = \arg\min_{\betav\in\mathbb{R}^p}\sum_{i=1}^{n}\|\yv-\Xmat\betav\|_2^2+\lambda\|\betav\|_2^2
\end{align*}
Here $\lambda$ is a tuning parameter which controls the strength of the penalty term. When $\lambda=0$, we get the linear regression estimate.
\paragraph{a. (5 pts)} Derive the closed form solution of $\hat{\betav}^{ridge}$. 
\paragraph{b. (15 pts)} Assume $n=50$ and $p=20$ and use the provided $\bm X$ as input.
% The entries of the predictor matrix $\Xmat\in\mathbb{R}^{50\times 20}$ are $i.i.d.$ $N(0,1)$. 
The response $\yv\in\mathbb{R}^{50}$ is drawn from the model $\yv=\Xmat\betav^*+\epsilonv$, where the entries of $\epsilonv\in\mathbb{R}^{50}$ are $i.i.d.$ $N(0,1)$.
The true regression coefficients are 

\noindent$\betav_1^*=(0.1,0.3,0.2,0.2,0.9,0.8,0.9,0.1,0.4,0.2,0.7,0.3,0.1,0.7,0.8,0.3,0.2,0.8,0.1,0.7)^T$, $\betav_2^*=(0.5,0.6,0.7,0.9,0.9,0.8,0.9,0.8,0.6,0.5,0.7,0.6,0.7,0.7,0.8,0.8,0.9,0.8,0.5,0.7)^T$.
Repeat the following $N = 100$ times: 1. Generate a response vector $\yv^{(n)}$ for $n=1,\cdots,N$; 2. Compute the estimated coefficients $\hat{\betav}^{(n)}$ use ridge regression; 
%3. Generate a new response $\hat{\yv}$ with $\hat{\beta}^{ridge}$; 
3. record the error $1/N\sum_{n=1}^{N}||\yv^{(n)}-\bm X^{(n)}\hat{\betav}^{(n)}||^2$. We average the observed error to get the estimated MSE.

Compute and compare the linear MSE for both $\betav_1$ and $\betav_2$. Plot the ridge MSE with respect to $\lambda$ for both $\betav_1$ and $\betav_2$. What do you find? Try to explain what you find. (Note: The error $\epsilonv$ should not be fixed during $100$ iterations, however it should be the same for $\betav_1$ and $\betav_2$ in each iteration so that we can compare.)

\paragraph{c. (20 pts)} This question aims to deal with the matrix inverse problem encountered in ridge regression. $\Xmat$ is the centered and standardized version of the previous question, i.e. $\Xmat^T \Xmat = \text{corr}(\Xmat)$. Use $\betav = \betav_1^*$. Suppose $ \Ymat = \bm 1 \alpha + \Umat_p \Lmat \Vmat^T \betav  + \epsilonv$, $\epsilonv \sim N(\bm 0, \Imat_n)$, where the data $\Xmat \in \mathbb{R}^{n \times p}$ is decomposed as $\Xmat = \Umat_p \Lmat \Vmat^T$ by singular value decomposition, where $\Umat_p \in \mathbb{R}^{n \times p}$, $\Lmat \in \mathbb{R}^{p \times p}$, $\Vmat \in \mathbb{R}^{p \times p}$ and $\Umat_p^T \bm U_p = \Imat_p$. $\Lmat$ is diagonal matrix. Let $\Umat = [\bm 1_n, \Umat_p , \Umat_{n-p-1}]$ be an $n \times n$ orthogonal matrix. Then we have $\Umat^T  \Ymat = \Umat^T \bm 1_n \alpha + \Umat^T \Umat_p \Lmat \Vmat^T \betav  + \Umat^T  \epsilonv$. If we further define $\Ymat^* = \Umat^T \Ymat$ and $\epsilonv^* = \Umat^T \epsilonv$, then

\begin{eqnarray*}
\Ymat^* = 
\begin{pmatrix}
n & \bm 0_p^T \\
\bm 0_{p} &  \Lmat \\
\bm 0_{n-p-1}  & \bm 0_{(n-p-1)\times p}
\end{pmatrix}
\begin{pmatrix}
\alpha \\
\bm \gamma
\end{pmatrix}
+ \epsilonv^*
\end{eqnarray*}

$\bm 0_p$ is a vector with all zero of length $p$ ($\bm 1_n$ is all one vector of length $n$). Calculate and write down the estimation of $\bm \gamma$ using ridge regression in closed form, denote as $\hat{\bm \gamma}$. $\lambda = 1$ and $\alpha = 0.1$. Run $N = 100$ times for different $\epsilonv$. Plot $\bm \gamma$ and $\mathbb{E}[\hat{\bm \gamma}]$ together, where $\mathbb{E}[\hat{\bm \gamma}] = \frac{1}{N} \sum_{n=1}^N \hat{\bm \gamma}^{(n)}$. Then on a different figure, plot $\frac{1}{N}\sum_{n=1}^N ( \hat{\gamma}_i^{(n)} - \gamma_i)^2$ and $\frac{l_i^2 + \gamma_i^2}{(l_i^2 + \lambda)^2}$ for $i=1,\cdots,p$ together. Note that $\bm \gamma = [\gamma_1, \gamma_2,\cdots, \gamma_p]$ and $\bm L = diag(l_1,l_2,\cdots,l_p)$.

\end{document}\grid
