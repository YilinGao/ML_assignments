\documentclass[paper=letter, fontsize=12pt]{article}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{mathrsfs}

%opening
\title{Compsci 571 HW6}
\author{Yilin Gao (yg95)}

\begin{document}

\maketitle
\section{Neural Networks and Universal Approximation Theorem}

\subsection{}

\begin{enumerate}[label=(\alph*)]
	\item The NN architecture is like following:
			
	\includegraphics[scale=0.4]{q1a.png}
			 
	The implementation is in \verb|q1.ipynb|. The approximated function is as following:
	
	\includegraphics[scale=0.6]{q1a2.png}
			 
	The minimal number of hidden neurons is 2, because the bump is a combination of 2 step functions, and each neuron (with the sigmoid activation function) is able to approximate one step function with any given step direction, location and height.
	
	\item In the NN, $w_{01}$ determines the steepness of the step-up part of the bump, $w_{02}$ determines the steepness of the step-down part of the bump. $-\frac{b_{01}}{w_{01}}$ determines the step-up location, $-\frac{b_{02}}{w_{02}}$ determines the step-down location. And $w_{13}$ and $w_{23}$ determine the height of the bump.
\end{enumerate}

\subsection{}

\begin{enumerate}[label=(\alph*)]
	\item 
	
	\item 
	
	\item 
\end{enumerate}

\section{EM}

\section{Clustering}
\begin{enumerate}[label=(\alph*)]
	\item See the implementation in \verb|q3.ipynb|.
	
	\item See the implementation in \verb|q3.ipynb|.
	
	\item The empirical clustering results on the dataset with both algorithms are as following:
	
	K-Means:
	
	\includegraphics[scale=0.6]{q3c1.png}
	
	Hierarchical Agglomerative Clustering:
	
	\includegraphics[scale=0.6]{q3c2.png}
	
	From the visualization, we can tell the hierarchical agglomerative clustering algorithm performs better. 
	
	On this specific dataset, the possible reason for the discrepancy is that K-Means assumes the variance of each X variable is spherical. However, the given data doesn't satisfy this assumption. 
	
	\item Possible preprocessing on data to make K-Means perform better: normalize each X variable to mean = 0 and variance = 1.
\end{enumerate}
\end{document}
